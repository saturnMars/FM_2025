{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saturnMars/FM_2025/blob/main/Lab4_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9965fa",
      "metadata": {
        "id": "ed9965fa"
      },
      "source": [
        "# Agent playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2f0e92",
      "metadata": {
        "id": "fc2f0e92"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain_openai langchain_huggingface langchain_community langchain_tavily"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a40ef73",
      "metadata": {
        "id": "5a40ef73"
      },
      "source": [
        "# Initialize the base language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1dda4be8",
      "metadata": {
        "id": "1dda4be8"
      },
      "outputs": [],
      "source": [
        "local_inference = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "055bb9cb",
      "metadata": {
        "id": "055bb9cb"
      },
      "source": [
        "### A) Cloud inference\n",
        "1. via [*Hugging Face’s Inference Providers*](https://huggingface.co/docs/inference-providers/en/index)\n",
        "    - Create an account for the Hugging Face platform: [huggingface.co/join](https://huggingface.co/join)\n",
        "    - Get the API key from dashboard: [huggingface.co/docs/hub/en/security-tokens](https://huggingface.co/docs/hub/en/security-tokens)\n",
        "2. via [*OpenAI API*](https://auth.openai.com)\n",
        "    - Create a new OpenAI account (free credits for new accounts): [auth.openai.com/create-account](https://auth.openai.com/create-account)\n",
        "    - Generate the API key from the dashboard: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f96e2cee",
      "metadata": {
        "id": "f96e2cee"
      },
      "outputs": [],
      "source": [
        "from os import environ\n",
        "environ[\"HF_TOKEN\"] = \"\"\n",
        "environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3c13a9de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c13a9de",
        "outputId": "d760dd1b-4279-4233-c8a3-29bb750f6ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloud inference: model: \"gpt-5-nano\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Cloud inference via OpenAI\n",
        "if not local_inference and environ.get('OPENAI_API_KEY'):\n",
        "    llm_model = ChatOpenAI(model=\"gpt-5-nano\", api_key=environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "    print(f\"Cloud inference: model: \\\"{llm_model.model_name}\\\"\")\n",
        "\n",
        "# Cloud inference via HuggingFace\n",
        "elif not local_inference and environ.get('HF_TOKEN'):\n",
        "    llm_model = ChatOpenAI(\n",
        "        base_url=\"https://router.huggingface.co/v1\",\n",
        "        model=\"CohereLabs/c4ai-command-r7b-12-2024\", # (1) Qwen/Qwen3-Next-80B-A3B-Instruct || (2) openai/gpt-oss-120b (3) CohereLabs/c4ai-command-r7b-12-2024\n",
        "        api_key=environ[\"HF_TOKEN\"])\n",
        "\n",
        "    print(f\"Cloud inference ({llm_model.openai_api_base}): model: \\\"{llm_model.model_name}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e9733c",
      "metadata": {
        "id": "e1e9733c"
      },
      "source": [
        "### B) Local inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1fc4d344",
      "metadata": {
        "id": "1fc4d344"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "if local_inference:\n",
        "    llm_model = ChatHuggingFace(\n",
        "        llm = HuggingFacePipeline.from_model_id(\n",
        "            model_id=\"allenai/OLMo-2-0425-1B-Instruct\", #  allenai/OLMo-2-0425-1B-Instruct | Qwen/Qwen3-4B-Instruct-2507\n",
        "            task =\"text-generation\",\n",
        "            pipeline_kwargs={'dtype':\"bfloat16\"}\n",
        "        ))\n",
        "    print(f\"Local Inference: \\\"{llm_model.llm.model_id}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d0becb",
      "metadata": {
        "id": "f7d0becb"
      },
      "source": [
        "# Initialize the tool\n",
        "This code demonstrates how to use the `TavilySearch` tool from the `langchain_tavily` package to perform a web search within a LangChain workflow.\n",
        "1. It imports the TavilySearch class, which is a tool designed to query the Tavily Search API and return structured search results, such as URLs, snippets, and optionally images or answers.\n",
        "2. The invoke method is then called with the query `\"What is Italy’s current public debt?\"`.\n",
        "    - This method sends the query to the Tavily API and returns the search results as a dictionary containing information such as the original query, a list of result items (with titles, URLs, and content snippets), and possibly other metadata.\n",
        "3. The results are printed to the output pane, allowing you to inspect the returned data.\n",
        "\n",
        "The code also shows how to organize tools for later use by placing the search tool into a list called tools. This is useful when building more complex agent workflows that may use multiple tools for different tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "28d76fe7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28d76fe7",
        "outputId": "12160ec7-66b6-4c99-f0e2-c4397d80b5d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"query\": \"What is Italy’s current public debt?\",\n",
            "    \"follow_up_questions\": null,\n",
            "    \"answer\": null,\n",
            "    \"images\": [],\n",
            "    \"results\": [\n",
            "        {\n",
            "            \"url\": \"https://en.wikipedia.org/wiki/Italian_government_debt\",\n",
            "            \"title\": \"Italian government debt - Wikipedia\",\n",
            "            \"content\": \"As of January 2014, the Italian government debt stands at €2.1 trillion (131.1% of GDP). Italy has the lowest share of public debt held by non-residents of all\",\n",
            "            \"score\": 0.91156644,\n",
            "            \"raw_content\": null\n",
            "        },\n",
            "        {\n",
            "            \"url\": \"https://www.reuters.com/markets/europe/italys-public-debt-tops-3-trillion-euros-highest-record-2025-01-15/\",\n",
            "            \"title\": \"Italy's public debt tops 3 trillion euros, highest on record | Reuters\",\n",
            "            \"content\": \"Italy's debt climbed to 3,005.2 billion euros in November, compared with 2,981.3 billion euros in the previous month, Bank of Italy data showed.\",\n",
            "            \"score\": 0.890761,\n",
            "            \"raw_content\": null\n",
            "        }\n",
            "    ],\n",
            "    \"response_time\": 0.99,\n",
            "    \"request_id\": \"b146119c-2dc4-4b86-8bd9-71ec24bdd392\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "from json import dumps\n",
        "\n",
        "# Initialize the Tavily Search tool\n",
        "search_tool = TavilySearch(max_results=2, tavily_api_key = \"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\")\n",
        "\n",
        "# Try out the search tool\n",
        "search_results = search_tool.invoke(input = \"What is Italy’s current public debt?\")\n",
        "print(dumps(search_results, indent=4, ensure_ascii = False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c049fa0a",
      "metadata": {
        "id": "c049fa0a"
      },
      "source": [
        "# Invoke the agent with a user query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cd983b4c",
      "metadata": {
        "id": "cd983b4c"
      },
      "outputs": [],
      "source": [
        "query = \"What's the weather like today in Trento, Italy?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13385960",
      "metadata": {
        "id": "13385960"
      },
      "source": [
        "### A) without the search tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06d66f60",
      "metadata": {
        "id": "06d66f60"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "29560cf9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29560cf9",
        "outputId": "e565416d-9206-4a4d-e225-9a079d936a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's the weather like today in Trento, Italy?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I can check the live weather for Trento, Italy. Would you like me to fetch the latest conditions now? I can report current temperature, sky conditions (sunny, cloudy, rain), humidity, wind, and a short forecast. If you prefer, I can also just give you a quick method to check it yourself.\n"
          ]
        }
      ],
      "source": [
        "# Define the input message\n",
        "input_message = {\"messages\": {\"role\": \"user\", \"content\": query}}\n",
        "\n",
        "# Invoke the agent\n",
        "response = agent_executor.invoke(input_message)\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9adb36",
      "metadata": {
        "id": "2a9adb36"
      },
      "source": [
        "### B) with the search tool\n",
        "![image.png](https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "84b98c8b",
      "metadata": {
        "id": "84b98c8b"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    tools = [search_tool],\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "82852a2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82852a2b",
        "outputId": "0b21816b-fa16-46a0-e762-4b76c9bc0736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's the weather like today in Trento, Italy?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search (call_hCSHTrrdfhKZMRLeggV0iRIW)\n",
            " Call ID: call_hCSHTrrdfhKZMRLeggV0iRIW\n",
            "  Args:\n",
            "    query: Trento weather today\n",
            "    time_range: day\n",
            "    include_images: False\n",
            "    search_depth: advanced\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search\n",
            "\n",
            "{\"query\": \"Trento weather today\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ventusky.com/trento\", \"title\": \"Weather - Trento - 14-Day Forecast & Rain | Ventusky\", \"content\": \"# Ventusky\\n\\nRadar\\n\\n# Trento\\n\\n46°3'N / 11°7'E / Altitude 197 m / 14:38 2025/11/10, Europe/Rome (UTC+1)\\n\\n|  |\\n\\n| 14 °C |\\n|  |\\n| Wind speed  4 km/h |\\n\\n|  |  |\\n --- |\\n| Humidity | 55 % |\\n\\nCalculated from nearby stations (13:50 2025/11/10)\\n\\nOpen Radar Map\\n\\n## Weather for the next 24 hours [...] | 01:00 | 04:00 | 07:00 | 10:00 | 13:00 | 16:00 | 19:00 | 22:00 |\\n ---  ---  ---  --- |\\n| partly cloudy 2 °C 0 mm 0 %  NW  1 km/h | clear sky 2 °C 0 mm 0 %  W  3 km/h | partly cloudy 2 °C 0 mm 0 %  NW  1 km/h | overcast 7 °C 0 mm 0 %  SW  1 km/h | partly cloudy 11 °C 0 mm 0 %  SE  1 km/h | clear sky 11 °C 0 mm 0 %  NE  1 km/h | clear sky 6 °C 0 mm 0 %  S  5 km/h | clear sky 4 °C 0 mm 0 %  S  3 km/h |\\n\\npartly cloudy\\nclear sky\\npartly cloudy\\novercast\\npartly cloudy\\nclear sky\\nclear sky\\nclear sky [...] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\\n ---  ---  ---  ---  ---  ---  --- |\\n| Mo 10 clear sky | Tu 11 partly cloudy | We 12 clear sky with few clouds | Th 13 clear sky with few clouds | Fr 14 partly cloudy | Sa 15 mostly cloudy | Su 16 clear sky with few clouds | Mo 17 mostly cloudy | Tu 18 overcast with rain | We 19 overcast with light rain | Th 20 mixed with rain showers | Fr 21 mixed with rain showers | Sa 22 clear sky with few clouds | Su 23 high clouds |\", \"score\": 0.8965509, \"raw_content\": null}, {\"url\": \"https://www.weatherbug.com/weather-forecast/hourly/cles-trentino-alto-adige-it?hour=20251110\", \"title\": \"Hourly Weather Forecast for Cles, Trentino-Alto Adige, IT\", \"content\": \"Mostly Clear\\n\\n## 11 PM\\n\\nTemperature\\nPrecipitation\\nWind\\n\\nPartly Cloudy\\n\\n## Mon, Nov 10\\n\\n### Top Stories\\n\\nSnowfall Forecast Through Tuesday\\n\\n#### Cold Temperatures and Lake-Effect Snow Galore\\n\\nAutumn snow will continue for portions of the Great Lakes and Northeast.\\n\\nToday's Weather Outlook\\n\\n#### Today's Weather Outlook\\n\\nWeekly Drought Map for November 6, 2025\\n\\n#### Beneficial Moisture Leads to Drought Improvements For N. Tier, East\\n\\nFall Colors This Weekend [...] # Hourly Weather Forecast - Cles, Trentino-Alto Adige, IT\\n\\n## Mon, Nov 10\\n\\n## 2 PM\\n\\nTemperature\\nPrecipitation\\nWind\\n\\nSunny\\n\\n## 3 PM\\n\\nTemperature\\nPrecipitation\\nWind\\n\\nSunny\\n\\n## 4 PM\\n\\nTemperature\\nPrecipitation\\nWind\\n\\nSunny\\n\\n## 5 PM\\n\\nTemperature\\nPrecipitation\\nWind\\n\\nClear\\n\\n## 6 PM\\n\\nTemperature\\nPrecipitation\\nWind\\n\\nClear\\n\\n## 7 PM\\n\\nTemperature\\nPrecipitation\\nWind\\n\\nClear\\n\\n## 8 PM\\n\\nTemperature\\nPrecipitation\\nWind\\n\\nClear\\n\\n## 9 PM\\n\\nTemperature\\nPrecipitation\\nWind\\n\\nClear\\n\\n## 10 PM\\n\\nTemperature\\nPrecipitation\\nWind [...] #### Foliage Update: November Continues Autumn's Glory\\n\\n(Image provided by NOAA)\\n\\n#### Polar Vortex: What Is It?\\n\\n2025-2026 Winter Outlook: Temperatures\\n\\n#### 2025-2026 Winter Outlook: La Niña Main Factor This Winter\\n\\nWeatherBug Logo\\nWeatherBug iOS App\\nWeatherBug Android App\", \"score\": 0.77487355, \"raw_content\": null}], \"response_time\": 4.21, \"request_id\": \"8d120073-dddd-4b3c-ac7a-9658efc9998b\"}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Here’s what I’m seeing for Trento, Italy right now:\n",
            "\n",
            "- Temperature: about 14°C\n",
            "- Humidity: ~55%\n",
            "- Wind: light, around 4 km/h\n",
            "- Sky: currently partly cloudy\n",
            "\n",
            "Today’s outlook (based on the same source):\n",
            "- The rest of the day looks cool but somewhat mild, with highs around 11–12°C.\n",
            "- Expect a mix of sun and clouds; skies clear later in the evening.\n",
            "- Tonight, temps fall to around 4–6°C with light winds.\n",
            "\n",
            "Source: Ventusky (Trento weather page) shows current conditions and the hourly forecast. Would you like an exact hourly forecast or a forecast from another source?\n"
          ]
        }
      ],
      "source": [
        "# Define the input message\n",
        "input_message = {\"messages\": {\"role\": \"user\", \"content\": query}}\n",
        "\n",
        "# Invoke the agent\n",
        "response = agent_executor.invoke(input_message)\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "535fc51a",
      "metadata": {
        "id": "535fc51a"
      },
      "source": [
        "# Create our custom tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7bb5f20a",
      "metadata": {
        "id": "7bb5f20a"
      },
      "outputs": [],
      "source": [
        "def get_exam_score(exam_name: str) -> dict:\n",
        "    \"\"\"Get the expected score for a given exam.\"\"\"\n",
        "\n",
        "    # For demonstration purposes, we assume a perfect score (we have high expectations!)\n",
        "    student_score = 30\n",
        "\n",
        "    return {\n",
        "        'exam_name': exam_name,\n",
        "        'range': (0, 30),\n",
        "        'score': student_score}\n",
        "\n",
        "def parse_result(score: int) -> dict:\n",
        "    \"\"\"Get the expected result (pass or fail) for a given score.\"\"\"\n",
        "\n",
        "    # For demonstration purposes, we assume a traditional passing threshold\n",
        "    pass_threshold = 18\n",
        "\n",
        "    # Context: exam is graded out of 30, with 18 as the passing threshold\n",
        "    results = {\n",
        "        'score': score,\n",
        "        'pass_threshold': pass_threshold,\n",
        "        'passed': score >= pass_threshold,\n",
        "        'cum_laude': False # sorry :/\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "29b01cad",
      "metadata": {
        "id": "29b01cad"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    tools = [get_exam_score, parse_result],\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d88a89a4",
      "metadata": {
        "id": "d88a89a4"
      },
      "outputs": [],
      "source": [
        "query = 'Will I ever pass the FM 2025 exam?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b030c85c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b030c85c",
        "outputId": "47c5e52d-85a6-4381-cd3a-d01a26260b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Will I ever pass the FM 2025 exam?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_exam_score (call_YNmJagBzpjGyCAFeF0GBRd1w)\n",
            " Call ID: call_YNmJagBzpjGyCAFeF0GBRd1w\n",
            "  Args:\n",
            "    exam_name: FM 2025\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_exam_score\n",
            "\n",
            "{\"exam_name\": \"FM 2025\", \"range\": [0, 30], \"score\": 30}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  parse_result (call_flPqPtTYYP1pg9oeSrf9UCng)\n",
            " Call ID: call_flPqPtTYYP1pg9oeSrf9UCng\n",
            "  Args:\n",
            "    score: 30\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: parse_result\n",
            "\n",
            "{\"score\": 30, \"pass_threshold\": 18, \"passed\": true, \"cum_laude\": false}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yes. Based on the latest result, you’ve already passed FM 2025.\n",
            "\n",
            "- Score: 30/30\n",
            "- Pass threshold: 18\n",
            "- Passed: True\n",
            "- Cum laude: False (not achieved on this attempt)\n",
            "\n",
            "If you want, I can help you with a study plan to aim for cum laude next time, or generate practice questions to reinforce topics.\n"
          ]
        }
      ],
      "source": [
        "response = agent_executor.invoke({\"messages\": {\"role\": \"user\", \"content\": query}})\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d92ddea",
      "metadata": {
        "id": "2d92ddea"
      },
      "source": [
        "# Human in the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9ce81553",
      "metadata": {
        "id": "9ce81553"
      },
      "outputs": [],
      "source": [
        "def parse_result(score: int) -> dict:\n",
        "    \"\"\"Get the expected result (pass or fail) for a given score.\"\"\"\n",
        "\n",
        "    # For demonstration purposes, we assume a traditional passing threshold\n",
        "    pass_threshold = 18\n",
        "\n",
        "    # Ask for human approval if the score is passing\n",
        "    accepted = None\n",
        "    if score >= pass_threshold:\n",
        "        user_input = input(f\"Do you accept a score equal to {score} (yes/no): \").strip().lower()\n",
        "        accepted = True if user_input == 'yes' else False\n",
        "\n",
        "    # Context: exam is graded out of 30, with 18 as the passing threshold\n",
        "    results = {\n",
        "        'score': score,\n",
        "        'pass_threshold': pass_threshold,\n",
        "        'passed': score >= pass_threshold,\n",
        "        'cum_laude': False, # sorry :/\n",
        "        'acceptedByStudent': accepted\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8e662af4",
      "metadata": {
        "id": "8e662af4"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    tools = [get_exam_score, parse_result],\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "48fb5c67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48fb5c67",
        "outputId": "a5f719dd-a597-48ca-acf7-3d9ba3506241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you accept a score equal to 30 (yes/no): no\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Will I ever pass the FM 2025 exam?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_exam_score (call_ymisxfVkJ8syLxJcvzhgsC5S)\n",
            " Call ID: call_ymisxfVkJ8syLxJcvzhgsC5S\n",
            "  Args:\n",
            "    exam_name: FM 2025 exam\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_exam_score\n",
            "\n",
            "{\"exam_name\": \"FM 2025 exam\", \"range\": [0, 30], \"score\": 30}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  parse_result (call_g3LnDDzmmoZyYtv2YnDduGHt)\n",
            " Call ID: call_g3LnDDzmmoZyYtv2YnDduGHt\n",
            "  Args:\n",
            "    score: 30\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: parse_result\n",
            "\n",
            "{\"score\": 30, \"pass_threshold\": 18, \"passed\": true, \"cum_laude\": false, \"acceptedByStudent\": false}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Short answer: Yes—you’ve passed.\n",
            "\n",
            "What the data shows:\n",
            "- Exam: FM 2025\n",
            "- Your score: 30 out of 30 (perfect)\n",
            "- Pass threshold: 18\n",
            "- Result: Passed (true)\n",
            "- Cum laude: false (not indicated as achieved)\n",
            "- Accepted by student: false (not applicable to the pass result)\n",
            "\n",
            "So, you’ve already met and exceeded the passing criteria. If you want, I can explain what cum laude would require or help plan next steps after passing.\n"
          ]
        }
      ],
      "source": [
        "response = agent_executor.invoke({\"messages\": {\"role\": \"user\", \"content\": query}}, config = {\"configurable\": {\"thread_id\": \"101\"}})\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677edcda",
      "metadata": {
        "id": "677edcda"
      },
      "source": [
        "# Conversational agents (i.e., ChatBots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947ec79e",
      "metadata": {
        "collapsed": true,
        "id": "947ec79e"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "63ea2a84",
      "metadata": {
        "id": "63ea2a84"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
        "from langchain_core.runnables import RunnableMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ad4b11d8",
      "metadata": {
        "id": "ad4b11d8"
      },
      "outputs": [],
      "source": [
        "store = {}\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    \"\"\"Retrieve or create chat history for a session.\"\"\"\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio"
      ],
      "metadata": {
        "id": "7v_nCtpAMOdy"
      },
      "id": "7v_nCtpAMOdy"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "QbTg_AKMiGro"
      },
      "id": "QbTg_AKMiGro",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "])\n",
        "\n",
        "# Chain the prompt with the LLM\n",
        "chain = prompt | llm_model\n",
        "\n",
        "# Create the chatbot with history support\n",
        "chatbot = RunnableWithMessageHistory(chain, get_session_history=get_session_history, input_messages_key=\"input\", history_messages_key=\"history\")"
      ],
      "metadata": {
        "id": "DPFkljz7M4EB"
      },
      "id": "DPFkljz7M4EB",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "def get_response(user_message, history):\n",
        "\n",
        "    # Execute the model\n",
        "    response = chatbot.invoke(input = {\"input\": user_message}, config={\"configurable\": {\"session_id\": session_id}})\n",
        "\n",
        "    # Get the text\n",
        "    generated_text = response.content\n",
        "\n",
        "    # Clear text\n",
        "    matches = list(re.finditer(user_message, generated_text))\n",
        "    if matches:\n",
        "        last_match = matches[-1]\n",
        "        generated_text = generated_text[last_match.end() + 1 :]\n",
        "\n",
        "    return generated_text.strip()\n",
        "\n",
        "# Define the interface\n",
        "session_id = 'user1'\n",
        "interface = gr.ChatInterface(fn=get_response, type=\"messages\", title=f\"ChatBot\", examples=[\"Tell me a joke\", \"What's the capital of France?\", \"What is the population of Trento?\"])\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch(share = True, debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "WpDoJIU1Ko7Q",
        "outputId": "3cc39362-15eb-4fcc-a8b9-da8bc711716d"
      },
      "id": "WpDoJIU1Ko7Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f7a85b973db3b3f93a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f7a85b973db3b3f93a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}