{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9965fa",
   "metadata": {},
   "source": [
    "# Agent playground\n",
    "![image.png](https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_openai langchain_tavily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40ef73",
   "metadata": {},
   "source": [
    "# Initialize the base language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda4be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_inference = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055bb9cb",
   "metadata": {},
   "source": [
    "### A) Cloud inference \n",
    "1. via [*Hugging Faceâ€™s Inference Providers*](https://huggingface.co/docs/inference-providers/en/index)\n",
    "    - Create an account for the Hugging Face platform: [huggingface.co/join](https://huggingface.co/join)\n",
    "    - Get the API key from dashboard: [huggingface.co/docs/hub/en/security-tokens](https://huggingface.co/docs/hub/en/security-tokens)\n",
    "2. via [*OpenAI API*](https://auth.openai.com)\n",
    "    - Create a new OpenAI account (free credits): [https://auth.openai.com/log-in](https://auth.openai.com/log-in)\n",
    "    - Generate the API key from the dashboard: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "environ[\"HF_TOKEN\"] = \"\"\n",
    "environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Cloud inference via OpenAI\n",
    "if not local_inference and environ.get('OPENAI_API_KEY'):\n",
    "    llm_model = ChatOpenAI(model=\"gpt-5-nano\", api_key=environ[\"OPENAI_API_KEY\"])\n",
    "    \n",
    "    print(f\"Cloud inference ({llm_model.openai_api_base}): model: \\\"{llm_model.model_name}\\\"\")\n",
    "    \n",
    "# Cloud inference via HuggingFace\n",
    "elif not local_inference and environ.get('HF_TOKEN'):\n",
    "    llm_model = ChatOpenAI(\n",
    "        base_url=\"https://router.huggingface.co/v1\",\n",
    "        model=\"Qwen/Qwen3-Next-80B-A3B-Instruct\", # (1) Qwen/Qwen3-Next-80B-A3B-Instruct || (2) openai/gpt-oss-120b\n",
    "        api_key=environ[\"HF_TOKEN\"])\n",
    "    \n",
    "    print(f\"Cloud inference ({llm_model.openai_api_base}): model: \\\"{llm_model.model_name}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e9733c",
   "metadata": {},
   "source": [
    "### B) Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "if local_inference:\n",
    "    llm_model = ChatHuggingFace(\n",
    "        llm = HuggingFacePipeline.from_model_id(\n",
    "            model_id=\"allenai/OLMo-2-0425-1B-Instruct\", #  Qwen/Qwen3-4B-Instruct-2507\n",
    "            task =\"text-generation\",\n",
    "            pipeline_kwargs={'dtype':\"bfloat16\"}\n",
    "        ))\n",
    "    print(f\"Local Inference: \\\"{llm_model.llm.model_id}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0becb",
   "metadata": {},
   "source": [
    "# Initialize the tool\n",
    "This code demonstrates how to use the `TavilySearch` tool from the `langchain_tavily` package to perform a web search within a LangChain workflow. \n",
    "1. It imports the TavilySearch class, which is a tool designed to query the Tavily Search API and return structured search results, such as URLs, snippets, and optionally images or answers.\n",
    "2. The invoke method is then called with the query `\"What is Italyâ€™s current public debt?\"`. \n",
    "    - This method sends the query to the Tavily API and returns the search results as a dictionary containing information such as the original query, a list of result items (with titles, URLs, and content snippets), and possibly other metadata.\n",
    "3. The results are printed to the output pane, allowing you to inspect the returned data. \n",
    "\n",
    "The code also shows how to organize tools for later use by placing the search tool into a list called tools. This is useful when building more complex agent workflows that may use multiple tools for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d76fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from json import dumps\n",
    "\n",
    "# Initialize the Tavily Search tool\n",
    "search_tool = TavilySearch(max_results=2, tavily_api_key = \"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\")\n",
    "\n",
    "# Try out the search tool\n",
    "#search_results = search_tool.invoke(input = \"What is Italyâ€™s current public debt?\")\n",
    "#print(dumps(search_results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049fa0a",
   "metadata": {},
   "source": [
    "# Invoke the agent with a user query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd983b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's the weather like today in Trento, Italy?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13385960",
   "metadata": {},
   "source": [
    "### A) without the search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d66f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "agent_executor = create_agent(\n",
    "    model = llm_model)\n",
    "    #system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29560cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input message\n",
    "input_message = {\"messages\": {\"role\": \"user\", \"content\": query}}\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent_executor.invoke(input_message)\n",
    "\n",
    "# Print the response\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9adb36",
   "metadata": {},
   "source": [
    "### B) with the search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b98c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_agent(\n",
    "    model = llm_model, \n",
    "    tools = [search_tool],\n",
    "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82852a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input message\n",
    "input_message = {\"messages\": {\"role\": \"user\", \"content\": query}}\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent_executor.invoke(input_message)\n",
    "\n",
    "# Print the response\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535fc51a",
   "metadata": {},
   "source": [
    "# Create our custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exam_score(exam_name: str) -> dict:\n",
    "    \"\"\"Get the expected score for a given exam.\"\"\"\n",
    "    \n",
    "    # For demonstration purposes, we assume a perfect score (we have high expectations!)\n",
    "    student_score = 30\n",
    "    \n",
    "    return {\n",
    "        'exam_name': exam_name, \n",
    "        'range': (0, 30), \n",
    "        'score': student_score}\n",
    "\n",
    "def parse_result(score: int) -> dict:\n",
    "    \"\"\"Get the expected result (pass or fail) for a given score.\"\"\"\n",
    "    \n",
    "    # For demonstration purposes, we assume a traditional passing threshold\n",
    "    pass_threshold = 18\n",
    "    \n",
    "    # Context: exam is graded out of 30, with 18 as the passing threshold\n",
    "    results = {\n",
    "        'score': score, \n",
    "        'pass_threshold': pass_threshold,\n",
    "        'passed': score >= pass_threshold,\n",
    "        'cum_laude': False # sorry :/\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b01cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_agent(\n",
    "    model = llm_model, \n",
    "    tools = [get_exam_score, parse_result],\n",
    "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efbc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for 5 seconds to avoid rate limiting issues\n",
    "import time \n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Will I ever pass the FM 2025 exam?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"messages\": {\"role\": \"user\", \"content\": query}})\n",
    "\n",
    "# Print the response\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92ddea",
   "metadata": {},
   "source": [
    "# Human in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce81553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result(score: int) -> dict:\n",
    "    \"\"\"Get the expected result (pass or fail) for a given score.\"\"\"\n",
    "    \n",
    "    # For demonstration purposes, we assume a traditional passing threshold\n",
    "    pass_threshold = 18\n",
    "    \n",
    "    # Ask for human approval if the score is passing\n",
    "    accepted = None\n",
    "    if score >= pass_threshold:\n",
    "        user_input = input(f\"Do you accept a score equal to {score} (yes/no): \").strip().lower()\n",
    "        accepted = True if user_input == 'yes' else False\n",
    "    \n",
    "    # Context: exam is graded out of 30, with 18 as the passing threshold\n",
    "    results = {\n",
    "        'score': score, \n",
    "        'pass_threshold': pass_threshold,\n",
    "        'passed': score >= pass_threshold,\n",
    "        'cum_laude': False, # sorry :/\n",
    "        'acceptedByStudent': accepted\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e662af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_agent(\n",
    "    model = llm_model, \n",
    "    tools = [get_exam_score, parse_result],\n",
    "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"messages\": {\"role\": \"user\", \"content\": query}}, config = {\"configurable\": {\"thread_id\": \"101\"}})\n",
    "\n",
    "# Print the response\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677edcda",
   "metadata": {},
   "source": [
    "# Conversetional agents (i.e., chat bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableMap\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Retrieve or create chat history for a session.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "def handle_command(user_input: str, session_id: str) -> tuple[str, str]:\n",
    "    \"\"\"Handle special commands and return (response, new_session_id).\"\"\"\n",
    "    if user_input.startswith(\"/clear\"):\n",
    "        store[session_id] = ChatMessageHistory()  # Reset history\n",
    "        return \"Chat history cleared.\", session_id\n",
    "    elif user_input.startswith(\"/session \"):\n",
    "        new_session = user_input.split(\" \", 1)[1].strip()\n",
    "        return f\"Switched to session '{new_session}'.\", new_session\n",
    "    elif user_input == \"/help\":\n",
    "        return (\"Commands: /clear (reset history), /session <id> (switch session), /exit (quit). Otherwise, just chat!\", session_id)\n",
    "    return None, session_id  # Not a command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3df475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond naturally and assist with queries.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessage(content=\"{input}\")##(\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Chain the prompt with the LLM\n",
    "chain = prompt | llm_model\n",
    "\n",
    "# Create the chatbot with history support\n",
    "chatbot = RunnableWithMessageHistory(chain, get_session_history=get_session_history, input_messages_key=\"input\", history_messages_key=\"history\")\n",
    "\n",
    "print(\"Chatbot ready! Type '/help' for commands or 'exit' to quit.\")\n",
    "session_id = \"user1\"\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if not user_input:\n",
    "            print(\"Bot: Please enter a message.\")\n",
    "            continue\n",
    "        \n",
    "        # Handle special commands\n",
    "        command_response, session_id = handle_command(user_input, session_id)\n",
    "        if command_response:\n",
    "            print(f\"Bot: {command_response}\")\n",
    "            continue\n",
    "        \n",
    "        # Exit condition\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Invoke the chatbot\n",
    "        result = chatbot.invoke(input = {\"input\": user_input}, config={\"configurable\": {\"session_id\": session_id}})\n",
    "        \n",
    "        # Print response with a separator\n",
    "        print(f\"Bot ({session_id}): {result.content}\")\n",
    "        time.sleep(0.5)  # Small delay for natural feel\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}. Please try again.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_ai(user_input, history=[]):\n",
    "    response = chatbot.invoke(input = {\"input\": user_input}, config={\"configurable\": {\"session_id\": session_id}})\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ec79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdb166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(message, history, session_id):\n",
    "    \"\"\"Handles a single user message and updates chat history.\"\"\"\n",
    "    response = chatbot.invoke(input = {\"input\": user_input}, config={\"configurable\": {\"session_id\": session_id}})\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create the Gradio chat interface\n",
    "interface = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"LangChain 1.0 Chatbot ðŸ¤–\",\n",
    "    description=\"A memory-aware chatbot built with LangChain 1.0 + OpenAI + Gradio.\",\n",
    "    examples=[\n",
    "        [\"Hello!\", \"user1\"],\n",
    "        [\"Tell me a joke about cats.\", \"user1\"],\n",
    "        [\"What's the capital of Japan?\", \"user2\"]\n",
    "    ],\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(label=\"Session ID\", value=\"user1\", info=\"Set a unique session name to keep context.\")\n",
    "    ])\n",
    "\n",
    "# Launch the Gradio interface with sharing enabled\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
