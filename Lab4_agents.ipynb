{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed9965fa",
      "metadata": {
        "id": "ed9965fa"
      },
      "source": [
        "# Agent playground\n",
        "![image.png](https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b9e8b402",
      "metadata": {
        "id": "b9e8b402"
      },
      "outputs": [],
      "source": [
        "#from os import environ\n",
        "#environ['CUDA_VISIBLE_DEVICES'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fc2f0e92",
      "metadata": {
        "id": "fc2f0e92",
        "outputId": "cebe7c05-9ab7-43f1-e30f-6ca3ebc9a100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab\n",
            "  Using cached colab-1.13.5.tar.gz (567 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U colab langchain langchain_openai langchain_huggingface langchain_community langchain_tavily"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a40ef73",
      "metadata": {
        "id": "5a40ef73"
      },
      "source": [
        "# Initialize the base language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1dda4be8",
      "metadata": {
        "id": "1dda4be8"
      },
      "outputs": [],
      "source": [
        "local_inference = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "055bb9cb",
      "metadata": {
        "id": "055bb9cb"
      },
      "source": [
        "### A) Cloud inference\n",
        "1. via [*Hugging Face’s Inference Providers*](https://huggingface.co/docs/inference-providers/en/index)\n",
        "    - Create an account for the Hugging Face platform: [huggingface.co/join](https://huggingface.co/join)\n",
        "    - Get the API key from dashboard: [huggingface.co/docs/hub/en/security-tokens](https://huggingface.co/docs/hub/en/security-tokens)\n",
        "2. via [*OpenAI API*](https://auth.openai.com)\n",
        "    - Create a new OpenAI account (free credits): [https://auth.openai.com/log-in](https://auth.openai.com/log-in)\n",
        "    - Generate the API key from the dashboard: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f96e2cee",
      "metadata": {
        "id": "f96e2cee"
      },
      "outputs": [],
      "source": [
        "from os import environ\n",
        "environ[\"HF_TOKEN\"] = \"\"\n",
        "environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3c13a9de",
      "metadata": {
        "id": "3c13a9de"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Cloud inference via OpenAI\n",
        "if not local_inference and environ.get('OPENAI_API_KEY'):\n",
        "    llm_model = ChatOpenAI(model=\"gpt-5-nano\", api_key=environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "    print(f\"Cloud inference ({llm_model.openai_api_base}): model: \\\"{llm_model.model_name}\\\"\")\n",
        "\n",
        "# Cloud inference via HuggingFace\n",
        "elif not local_inference and environ.get('HF_TOKEN'):\n",
        "    llm_model = ChatOpenAI(\n",
        "        base_url=\"https://router.huggingface.co/v1\",\n",
        "        model=\"Qwen/Qwen3-Next-80B-A3B-Instruct\", # (1) Qwen/Qwen3-Next-80B-A3B-Instruct || (2) openai/gpt-oss-120b\n",
        "        api_key=environ[\"HF_TOKEN\"])\n",
        "\n",
        "    print(f\"Cloud inference ({llm_model.openai_api_base}): model: \\\"{llm_model.model_name}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e9733c",
      "metadata": {
        "id": "e1e9733c"
      },
      "source": [
        "### B) Local inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1fc4d344",
      "metadata": {
        "id": "1fc4d344",
        "outputId": "51dc71df-ebce-427c-edbb-d1420d11c30e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Inference: \"allenai/OLMo-2-0425-1B-Instruct\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "if local_inference:\n",
        "    llm_model = ChatHuggingFace(\n",
        "        llm = HuggingFacePipeline.from_model_id(\n",
        "            model_id=\"allenai/OLMo-2-0425-1B-Instruct\", #  Qwen/Qwen3-4B-Instruct-2507\n",
        "            task =\"text-generation\",\n",
        "            pipeline_kwargs={'dtype':\"bfloat16\"}\n",
        "        ))\n",
        "    print(f\"Local Inference: \\\"{llm_model.llm.model_id}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d0becb",
      "metadata": {
        "id": "f7d0becb"
      },
      "source": [
        "# Initialize the tool\n",
        "This code demonstrates how to use the `TavilySearch` tool from the `langchain_tavily` package to perform a web search within a LangChain workflow.\n",
        "1. It imports the TavilySearch class, which is a tool designed to query the Tavily Search API and return structured search results, such as URLs, snippets, and optionally images or answers.\n",
        "2. The invoke method is then called with the query `\"What is Italy’s current public debt?\"`.\n",
        "    - This method sends the query to the Tavily API and returns the search results as a dictionary containing information such as the original query, a list of result items (with titles, URLs, and content snippets), and possibly other metadata.\n",
        "3. The results are printed to the output pane, allowing you to inspect the returned data.\n",
        "\n",
        "The code also shows how to organize tools for later use by placing the search tool into a list called tools. This is useful when building more complex agent workflows that may use multiple tools for different tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "28d76fe7",
      "metadata": {
        "id": "28d76fe7"
      },
      "outputs": [],
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "from json import dumps\n",
        "\n",
        "# Initialize the Tavily Search tool\n",
        "search_tool = TavilySearch(max_results=2, tavily_api_key = \"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\")\n",
        "\n",
        "# Try out the search tool\n",
        "#search_results = search_tool.invoke(input = \"What is Italy’s current public debt?\")\n",
        "#print(dumps(search_results, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c049fa0a",
      "metadata": {
        "id": "c049fa0a"
      },
      "source": [
        "# Invoke the agent with a user query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cd983b4c",
      "metadata": {
        "id": "cd983b4c"
      },
      "outputs": [],
      "source": [
        "query = \"What's the weather like today in Trento, Italy?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13385960",
      "metadata": {
        "id": "13385960"
      },
      "source": [
        "### A) without the search tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "06d66f60",
      "metadata": {
        "id": "06d66f60"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "agent_executor = create_agent(\n",
        "    model = llm_model)\n",
        "    #system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "29560cf9",
      "metadata": {
        "id": "29560cf9",
        "outputId": "b3e98e9a-51b8-433d-cbcf-3d6f162a5601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's the weather like today in Trento, Italy?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "<|endoftext|><|user|>\n",
            "What's the weather like today in Trento, Italy?\n",
            "<|assistant|>\n",
            "As an AI, I don't have real-time access to current weather data. For the most accurate information, please check a reliable weather website, a local weather station, or use a weather app on your mobile device. For Trento, Italy, you can look up current weather conditions on sites like AccuWeather, Forecasts.co.uk, or any reliable national weather service. As of my last update, the weather for Trento was typically characterized by being cold in winter and temperate in the summer, with occasional rainy periods.\n"
          ]
        }
      ],
      "source": [
        "# Define the input message\n",
        "input_message = {\"messages\": {\"role\": \"user\", \"content\": query}}\n",
        "\n",
        "# Invoke the agent\n",
        "response = agent_executor.invoke(input_message)\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9adb36",
      "metadata": {
        "id": "2a9adb36"
      },
      "source": [
        "### B) with the search tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "84b98c8b",
      "metadata": {
        "id": "84b98c8b"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    tools = [search_tool],\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "82852a2b",
      "metadata": {
        "id": "82852a2b",
        "outputId": "0acde3d8-aa22-4887-c60f-e80c2504aa96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's the weather like today in Trento, Italy?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "<|endoftext|><|system|>\n",
            "You are a helpful assistant that exploits all available tools to find up-to-date information.\n",
            "<|user|>\n",
            "What's the weather like today in Trento, Italy?\n",
            "<|assistant|>\n",
            "As of my knowledge cutoff in 2023, I can tell you that today in Trento, Italy, the weather was mostly sunny with a high of 17°C (63°F), and a low of 5°C (41°F). Please note that weather can change rapidly, so it's always a good idea to check a reliable source or a real-time weather app for the most current information.\n"
          ]
        }
      ],
      "source": [
        "# Define the input message\n",
        "input_message = {\"messages\": {\"role\": \"user\", \"content\": query}}\n",
        "\n",
        "# Invoke the agent\n",
        "response = agent_executor.invoke(input_message)\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "535fc51a",
      "metadata": {
        "id": "535fc51a"
      },
      "source": [
        "# Create our custom tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7bb5f20a",
      "metadata": {
        "id": "7bb5f20a"
      },
      "outputs": [],
      "source": [
        "def get_exam_score(exam_name: str) -> dict:\n",
        "    \"\"\"Get the expected score for a given exam.\"\"\"\n",
        "\n",
        "    # For demonstration purposes, we assume a perfect score (we have high expectations!)\n",
        "    student_score = 30\n",
        "\n",
        "    return {\n",
        "        'exam_name': exam_name,\n",
        "        'range': (0, 30),\n",
        "        'score': student_score}\n",
        "\n",
        "def parse_result(score: int) -> dict:\n",
        "    \"\"\"Get the expected result (pass or fail) for a given score.\"\"\"\n",
        "\n",
        "    # For demonstration purposes, we assume a traditional passing threshold\n",
        "    pass_threshold = 18\n",
        "\n",
        "    # Context: exam is graded out of 30, with 18 as the passing threshold\n",
        "    results = {\n",
        "        'score': score,\n",
        "        'pass_threshold': pass_threshold,\n",
        "        'passed': score >= pass_threshold,\n",
        "        'cum_laude': False # sorry :/\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "29b01cad",
      "metadata": {
        "id": "29b01cad"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    tools = [get_exam_score, parse_result],\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7efbc340",
      "metadata": {
        "id": "7efbc340"
      },
      "outputs": [],
      "source": [
        "# Wait for 5 seconds to avoid rate limiting issues\n",
        "import time\n",
        "time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d88a89a4",
      "metadata": {
        "id": "d88a89a4"
      },
      "outputs": [],
      "source": [
        "query = 'Will I ever pass the FM 2025 exam?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b030c85c",
      "metadata": {
        "id": "b030c85c",
        "outputId": "009caf02-9a94-4baa-faa8-172dd5d33d8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Will I ever pass the FM 2025 exam?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "<|endoftext|><|system|>\n",
            "You are a helpful assistant that exploits all available tools to find up-to-date information.\n",
            "<|user|>\n",
            "Will I ever pass the FM 2025 exam?\n",
            "<|assistant|>\n",
            "As of my last update (March 2023), FM 2025 is a comprehensive course that covers a broad spectrum of finance and management topics, including accounting, managerial accounting, financial analysis, and strategic management. While I can't provide an exact pass rate or guarantee your success, I can offer some general advice:\n",
            "\n",
            "1. **Understand the Content**: Make sure you thoroughly review all the subject matter covered in the course. Check that you understand the key concepts, terminologies, and examples presented.\n",
            "\n",
            "2. **Practice and Take Quizzes**: Use past exam questions to test your knowledge. Understand the types of questions you might encounter.\n",
            "\n",
            "3. **Get Continuous Learning**: Keep up-to-date with the latest trends in finance and management. The field is constantly evolving, with new regulations, technologies, and approaches emerging.\n",
            "\n",
            "4. **Use Official Sources**: The course's provider, FM Education, should provide official preparation materials. These are designed to be accurate and aligned with the exam's specifications.\n",
            "\n",
            "5. **Study Groups**: Join online forums or study groups. Sharing your doubts and explaining concepts to others can reinforce your understanding.\n",
            "\n",
            "6. **Workshops and Webinars**: Participate in workshops and webinars offered by FM Education or other reputable providers. These sessions can provide insights into\n"
          ]
        }
      ],
      "source": [
        "response = agent_executor.invoke({\"messages\": {\"role\": \"user\", \"content\": query}})\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d92ddea",
      "metadata": {
        "id": "2d92ddea"
      },
      "source": [
        "# Human in the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9ce81553",
      "metadata": {
        "id": "9ce81553"
      },
      "outputs": [],
      "source": [
        "def parse_result(score: int) -> dict:\n",
        "    \"\"\"Get the expected result (pass or fail) for a given score.\"\"\"\n",
        "\n",
        "    # For demonstration purposes, we assume a traditional passing threshold\n",
        "    pass_threshold = 18\n",
        "\n",
        "    # Ask for human approval if the score is passing\n",
        "    accepted = None\n",
        "    if score >= pass_threshold:\n",
        "        user_input = input(f\"Do you accept a score equal to {score} (yes/no): \").strip().lower()\n",
        "        accepted = True if user_input == 'yes' else False\n",
        "\n",
        "    # Context: exam is graded out of 30, with 18 as the passing threshold\n",
        "    results = {\n",
        "        'score': score,\n",
        "        'pass_threshold': pass_threshold,\n",
        "        'passed': score >= pass_threshold,\n",
        "        'cum_laude': False, # sorry :/\n",
        "        'acceptedByStudent': accepted\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8e662af4",
      "metadata": {
        "id": "8e662af4"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    tools = [get_exam_score, parse_result],\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "48fb5c67",
      "metadata": {
        "id": "48fb5c67",
        "outputId": "db06d231-e55e-465a-ab90-12d2fd2eb926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Will I ever pass the FM 2025 exam?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "<|endoftext|><|system|>\n",
            "You are a helpful assistant that exploits all available tools to find up-to-date information.\n",
            "<|user|>\n",
            "Will I ever pass the FM 2025 exam?\n",
            "<|assistant|>\n",
            "Unfortunately, I cannot provide you with a guarantee on passing the FM 2025 exam as it depends on various factors such as your personal strengths, knowledge, and experience. However, I can offer you some tips that may help you prepare for the exam:\n",
            "\n",
            "1. Understand the concepts: Familiarize yourself with the key topics and concepts covered in FM 2025, as they are the foundation of the exam.\n",
            "\n",
            "2. Study material: Review the official FM 2025 study guides and materials provided by the Army Institute of Military Studies (AIMS) or other approved resources.\n",
            "\n",
            "3. Practice exams: Take practice exams to familiarize yourself with the format and types of questions asked in the exam. This will help you build your test-taking skills and improve your score.\n",
            "\n",
            "4. Seek help from experienced professionals: Consult with experienced FM 2025 tutors or professionals who can provide you with personalized study plans and tips.\n",
            "\n",
            "5. Attend training: Enroll in an FM 2025 training course which can help you understand the material better and reinforce your knowledge.\n",
            "\n",
            "6. Schedule regular review sessions: Continuously review and refresh your knowledge of the topics covered in FM 2025 regularly to ensure your understanding remains current.\n",
            "\n",
            "While I cannot guarantee your passing on the first try, following the tips\n"
          ]
        }
      ],
      "source": [
        "response = agent_executor.invoke({\"messages\": {\"role\": \"user\", \"content\": query}}, config = {\"configurable\": {\"thread_id\": \"101\"}})\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677edcda",
      "metadata": {
        "id": "677edcda"
      },
      "source": [
        "# Conversetional agents (i.e., chat bot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "947ec79e",
      "metadata": {
        "collapsed": true,
        "id": "947ec79e",
        "outputId": "25414502-f3a0-4422-88f4-a8cfc41e77e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "63ea2a84",
      "metadata": {
        "id": "63ea2a84"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
        "from langchain_core.runnables import RunnableMap\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ad4b11d8",
      "metadata": {
        "id": "ad4b11d8"
      },
      "outputs": [],
      "source": [
        "store = {}\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    \"\"\"Retrieve or create chat history for a session.\"\"\"\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio"
      ],
      "metadata": {
        "id": "7v_nCtpAMOdy"
      },
      "id": "7v_nCtpAMOdy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Respond naturally and assist with queries.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Chain the prompt with the LLM\n",
        "chain = prompt | llm_model\n",
        "\n",
        "# Create the chatbot with history support\n",
        "chatbot = RunnableWithMessageHistory(chain, get_session_history=get_session_history, input_messages_key=\"input\", history_messages_key=\"history\")"
      ],
      "metadata": {
        "id": "DPFkljz7M4EB"
      },
      "id": "DPFkljz7M4EB",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def get_response(message, history, session_id):\n",
        "    \"\"\"Handles a single user message and updates chat history.\"\"\"\n",
        "    response = chatbot.invoke(input = {\"input\": message}, config={\"configurable\": {\"session_id\": session_id}})\n",
        "    history.append(TMP)\n",
        "    return response.content.strip()\n",
        "\n",
        "demo = gr.ChatInterface(fn=get_response, type=\"messages\", title=f\"ChatBot ({llm_model.model_id})\",  #\n",
        "                        examples=[[\"Tell me a joke\"]],\n",
        "                        additional_inputs=[gr.Textbox(label=\"Session ID\", value=\"user1\", info=\"Each session ID has its own conversation memory.\")])\n",
        "demo.launch(share = True, debug = True)"
      ],
      "metadata": {
        "id": "WpDoJIU1Ko7Q",
        "outputId": "873b389e-8496-4a33-dafc-08e717551cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "id": "WpDoJIU1Ko7Q",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3adcae5c963c7e160c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3adcae5c963c7e160c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2127, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1904, in postprocess_data\n",
            "    prediction_value = block.postprocess(prediction_value)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/components/chatbot.py\", line 634, in postprocess\n",
            "    self._check_format(value, \"messages\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/components/chatbot.py\", line 422, in _check_format\n",
            "    raise Error(\n",
            "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3adcae5c963c7e160c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}