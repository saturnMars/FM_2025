{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saturnMars/FM_2025/blob/main/Lab4_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9965fa",
      "metadata": {
        "id": "ed9965fa"
      },
      "source": [
        "# Agent playground\n",
        "![image.png](https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2f0e92",
      "metadata": {
        "id": "fc2f0e92"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain_openai langchain_huggingface langchain_community langchain_tavily"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a40ef73",
      "metadata": {
        "id": "5a40ef73"
      },
      "source": [
        "# Initialize the base language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1dda4be8",
      "metadata": {
        "id": "1dda4be8"
      },
      "outputs": [],
      "source": [
        "local_inference = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "055bb9cb",
      "metadata": {
        "id": "055bb9cb"
      },
      "source": [
        "### A) Cloud inference\n",
        "1. via [*Hugging Face’s Inference Providers*](https://huggingface.co/docs/inference-providers/en/index)\n",
        "    - Create an account for the Hugging Face platform: [huggingface.co/join](https://huggingface.co/join)\n",
        "    - Get the API key from dashboard: [huggingface.co/docs/hub/en/security-tokens](https://huggingface.co/docs/hub/en/security-tokens)\n",
        "2. via [*OpenAI API*](https://auth.openai.com)\n",
        "    - Create a new OpenAI account (free credits): [auth.openai.com/create-account](https://auth.openai.com/create-account)\n",
        "    - Generate the API key from the dashboard: [platform.openai.com/api-keys](https://platform.openai.com/api-keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f96e2cee",
      "metadata": {
        "id": "f96e2cee"
      },
      "outputs": [],
      "source": [
        "from os import environ\n",
        "environ[\"HF_TOKEN\"] = \"\"\n",
        "environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3c13a9de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c13a9de",
        "outputId": "39a86087-a88e-47aa-d616-fa9fe2091633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloud inference (https://router.huggingface.co/v1): model: \"CohereLabs/c4ai-command-r7b-12-2024\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Cloud inference via OpenAI\n",
        "if not local_inference and environ.get('OPENAI_API_KEY'):\n",
        "    llm_model = ChatOpenAI(model=\"gpt-5-nano\", api_key=environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "    print(f\"Cloud inference: model: \\\"{llm_model.model_name}\\\"\")\n",
        "\n",
        "# Cloud inference via HuggingFace\n",
        "elif not local_inference and environ.get('HF_TOKEN'):\n",
        "    llm_model = ChatOpenAI(\n",
        "        base_url=\"https://router.huggingface.co/v1\",\n",
        "        model=\"CohereLabs/c4ai-command-r7b-12-2024\", # (1) Qwen/Qwen3-Next-80B-A3B-Instruct || (2) openai/gpt-oss-120b (3) CohereLabs/c4ai-command-r7b-12-2024\n",
        "        api_key=environ[\"HF_TOKEN\"])\n",
        "\n",
        "    print(f\"Cloud inference ({llm_model.openai_api_base}): model: \\\"{llm_model.model_name}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e9733c",
      "metadata": {
        "id": "e1e9733c"
      },
      "source": [
        "### B) Local inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1fc4d344",
      "metadata": {
        "id": "1fc4d344"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "\n",
        "if local_inference:\n",
        "    llm_model = ChatHuggingFace(\n",
        "        llm = HuggingFacePipeline.from_model_id(\n",
        "            model_id=\"allenai/OLMo-2-0425-1B-Instruct\", #  allenai/OLMo-2-0425-1B-Instruct | Qwen/Qwen3-4B-Instruct-2507\n",
        "            task =\"text-generation\",\n",
        "            pipeline_kwargs={'dtype':\"bfloat16\"}\n",
        "        ))\n",
        "    print(f\"Local Inference: \\\"{llm_model.llm.model_id}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d0becb",
      "metadata": {
        "id": "f7d0becb"
      },
      "source": [
        "# Initialize the tool\n",
        "This code demonstrates how to use the `TavilySearch` tool from the `langchain_tavily` package to perform a web search within a LangChain workflow.\n",
        "1. It imports the TavilySearch class, which is a tool designed to query the Tavily Search API and return structured search results, such as URLs, snippets, and optionally images or answers.\n",
        "2. The invoke method is then called with the query `\"What is Italy’s current public debt?\"`.\n",
        "    - This method sends the query to the Tavily API and returns the search results as a dictionary containing information such as the original query, a list of result items (with titles, URLs, and content snippets), and possibly other metadata.\n",
        "3. The results are printed to the output pane, allowing you to inspect the returned data.\n",
        "\n",
        "The code also shows how to organize tools for later use by placing the search tool into a list called tools. This is useful when building more complex agent workflows that may use multiple tools for different tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "28d76fe7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28d76fe7",
        "outputId": "d10e3cc5-d766-4257-ae7d-086286f23a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"query\": \"What is Italy’s current public debt?\",\n",
            "    \"follow_up_questions\": null,\n",
            "    \"answer\": null,\n",
            "    \"images\": [],\n",
            "    \"results\": [\n",
            "        {\n",
            "            \"url\": \"https://en.wikipedia.org/wiki/Italian_government_debt\",\n",
            "            \"title\": \"Italian government debt - Wikipedia\",\n",
            "            \"content\": \"As of January 2014, the Italian government debt stands at €2.1 trillion (131.1% of GDP). Italy has the lowest share of public debt held by non-residents of all\",\n",
            "            \"score\": 0.91156644,\n",
            "            \"raw_content\": null\n",
            "        },\n",
            "        {\n",
            "            \"url\": \"https://www.reuters.com/markets/europe/italys-public-debt-tops-3-trillion-euros-highest-record-2025-01-15/\",\n",
            "            \"title\": \"Italy's public debt tops 3 trillion euros, highest on record | Reuters\",\n",
            "            \"content\": \"Italy's debt climbed to 3,005.2 billion euros in November, compared with 2,981.3 billion euros in the previous month, Bank of Italy data showed.\",\n",
            "            \"score\": 0.890761,\n",
            "            \"raw_content\": null\n",
            "        }\n",
            "    ],\n",
            "    \"response_time\": 1.11,\n",
            "    \"request_id\": \"bf4b5ef4-1308-4fa5-94bd-151f2c8241d2\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "from json import dumps\n",
        "\n",
        "# Initialize the Tavily Search tool\n",
        "search_tool = TavilySearch(max_results=2, tavily_api_key = \"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\")\n",
        "\n",
        "# Try out the search tool\n",
        "search_results = search_tool.invoke(input = \"What is Italy’s current public debt?\")\n",
        "print(dumps(search_results, indent=4, ensure_ascii = False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c049fa0a",
      "metadata": {
        "id": "c049fa0a"
      },
      "source": [
        "# Invoke the agent with a user query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cd983b4c",
      "metadata": {
        "id": "cd983b4c"
      },
      "outputs": [],
      "source": [
        "query = \"What's the weather like today in Trento, Italy?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13385960",
      "metadata": {
        "id": "13385960"
      },
      "source": [
        "### A) without the search tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "06d66f60",
      "metadata": {
        "id": "06d66f60"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "29560cf9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29560cf9",
        "outputId": "fffcf042-cd81-4a92-dcd1-0e19f9400700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's the weather like today in Trento, Italy?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm sorry, I don't have access to real-time weather data. However, you can check the weather in Trento, Italy, using various weather websites or applications. These sources will provide you with the most up-to-date and accurate information about the current weather conditions in Trento.\n"
          ]
        }
      ],
      "source": [
        "# Define the input message\n",
        "input_message = {\"messages\": {\"role\": \"user\", \"content\": query}}\n",
        "\n",
        "# Invoke the agent\n",
        "response = agent_executor.invoke(input_message)\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9adb36",
      "metadata": {
        "id": "2a9adb36"
      },
      "source": [
        "### B) with the search tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "84b98c8b",
      "metadata": {
        "id": "84b98c8b"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    tools = [search_tool],\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "82852a2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82852a2b",
        "outputId": "57b76b89-5bcb-4875-92e7-e565a12771d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's the weather like today in Trento, Italy?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search (tavily_search0)\n",
            " Call ID: tavily_search0\n",
            "  Args:\n",
            "    query: weather in Trento, Italy today\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search\n",
            "\n",
            "{\"query\": \"weather in Trento, Italy today\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in Trento, Italy\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'Trento', 'region': 'Trentino-Alto Adige', 'country': 'Italy', 'lat': 46.0667, 'lon': 11.1333, 'tz_id': 'Europe/Rome', 'localtime_epoch': 1762780943, 'localtime': '2025-11-10 14:22'}, 'current': {'last_updated_epoch': 1762780500, 'last_updated': '2025-11-10 14:15', 'temp_c': 14.4, 'temp_f': 57.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 2.2, 'wind_kph': 3.6, 'wind_degree': 213, 'wind_dir': 'SSW', 'pressure_mb': 1014.0, 'pressure_in': 29.94, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 55, 'cloud': 0, 'feelslike_c': 15.1, 'feelslike_f': 59.2, 'windchill_c': 11.0, 'windchill_f': 51.7, 'heatindex_c': 11.0, 'heatindex_f': 51.7, 'dewpoint_c': 2.2, 'dewpoint_f': 35.9, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 1.3, 'gust_mph': 6.7, 'gust_kph': 10.8}}\", \"score\": 0.9961973, \"raw_content\": null}, {\"url\": \"https://wisemeteo.com/en/country/italy/region/trentino-south-tyrol/city/trento/date/11-10\", \"title\": \"Trento, Trentino-South Tyrol - weather outlook for 10 November 2025\", \"content\": \"Trento, Trentino-South Tyrol - weather forecast for 10 November 2025 ; 0:00. Rain. 41° ; 3:00. Cloudy. 41° ; 6:00. Cloudy night. 37° ; 9:00. Cloudy sun. 39° ; 12:00.\", \"score\": 0.94956195, \"raw_content\": null}], \"response_time\": 2.16, \"request_id\": \"6da326e1-6eda-444e-96bf-d20aa976b90a\"}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Today in Trento, Italy, the weather is sunny with a temperature of 14.4°C and a humidity of 55%.\n",
            "\n",
            "However, there is also a forecast of rain and cloudy periods for later today.\n"
          ]
        }
      ],
      "source": [
        "# Define the input message\n",
        "input_message = {\"messages\": {\"role\": \"user\", \"content\": query}}\n",
        "\n",
        "# Invoke the agent\n",
        "response = agent_executor.invoke(input_message)\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "535fc51a",
      "metadata": {
        "id": "535fc51a"
      },
      "source": [
        "# Create our custom tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7bb5f20a",
      "metadata": {
        "id": "7bb5f20a"
      },
      "outputs": [],
      "source": [
        "def get_exam_score(exam_name: str) -> dict:\n",
        "    \"\"\"Get the expected score for a given exam.\"\"\"\n",
        "\n",
        "    # For demonstration purposes, we assume a perfect score (we have high expectations!)\n",
        "    student_score = 30\n",
        "\n",
        "    return {\n",
        "        'exam_name': exam_name,\n",
        "        'range': (0, 30),\n",
        "        'score': student_score}\n",
        "\n",
        "def parse_result(score: int) -> dict:\n",
        "    \"\"\"Get the expected result (pass or fail) for a given score.\"\"\"\n",
        "\n",
        "    # For demonstration purposes, we assume a traditional passing threshold\n",
        "    pass_threshold = 18\n",
        "\n",
        "    # Context: exam is graded out of 30, with 18 as the passing threshold\n",
        "    results = {\n",
        "        'score': score,\n",
        "        'pass_threshold': pass_threshold,\n",
        "        'passed': score >= pass_threshold,\n",
        "        'cum_laude': False # sorry :/\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "29b01cad",
      "metadata": {
        "id": "29b01cad"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    tools = [get_exam_score, parse_result],\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d88a89a4",
      "metadata": {
        "id": "d88a89a4"
      },
      "outputs": [],
      "source": [
        "query = 'Will I ever pass the FM 2025 exam?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b030c85c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b030c85c",
        "outputId": "de4d9235-3b12-4f51-ff08-6089311845c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Will I ever pass the FM 2025 exam?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_exam_score (get_exam_score0)\n",
            " Call ID: get_exam_score0\n",
            "  Args:\n",
            "    exam_name: FM 2025\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_exam_score\n",
            "\n",
            "{\"exam_name\": \"FM 2025\", \"range\": [0, 30], \"score\": 30}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  parse_result (parse_result0)\n",
            " Call ID: parse_result0\n",
            "  Args:\n",
            "    score: 30\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: parse_result\n",
            "\n",
            "{\"score\": 30, \"pass_threshold\": 18, \"passed\": true, \"cum_laude\": false}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yes, you will pass the FM 2025 exam. The expected score is 30, which is above the pass threshold of 18.\n"
          ]
        }
      ],
      "source": [
        "response = agent_executor.invoke({\"messages\": {\"role\": \"user\", \"content\": query}})\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d92ddea",
      "metadata": {
        "id": "2d92ddea"
      },
      "source": [
        "# Human in the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9ce81553",
      "metadata": {
        "id": "9ce81553"
      },
      "outputs": [],
      "source": [
        "def parse_result(score: int) -> dict:\n",
        "    \"\"\"Get the expected result (pass or fail) for a given score.\"\"\"\n",
        "\n",
        "    # For demonstration purposes, we assume a traditional passing threshold\n",
        "    pass_threshold = 18\n",
        "\n",
        "    # Ask for human approval if the score is passing\n",
        "    accepted = None\n",
        "    if score >= pass_threshold:\n",
        "        user_input = input(f\"Do you accept a score equal to {score} (yes/no): \").strip().lower()\n",
        "        accepted = True if user_input == 'yes' else False\n",
        "\n",
        "    # Context: exam is graded out of 30, with 18 as the passing threshold\n",
        "    results = {\n",
        "        'score': score,\n",
        "        'pass_threshold': pass_threshold,\n",
        "        'passed': score >= pass_threshold,\n",
        "        'cum_laude': False, # sorry :/\n",
        "        'acceptedByStudent': accepted\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8e662af4",
      "metadata": {
        "id": "8e662af4"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_agent(\n",
        "    model = llm_model,\n",
        "    tools = [get_exam_score, parse_result],\n",
        "    system_prompt = \"You are a helpful assistant that exploits all available tools to find up-to-date information.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "48fb5c67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48fb5c67",
        "outputId": "15185660-623f-48c3-e4e5-00932cff6d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you accept a score equal to 30 (yes/no): yes\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Will I ever pass the FM 2025 exam?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_exam_score (get_exam_score0)\n",
            " Call ID: get_exam_score0\n",
            "  Args:\n",
            "    exam_name: FM 2025\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_exam_score\n",
            "\n",
            "{\"exam_name\": \"FM 2025\", \"range\": [0, 30], \"score\": 30}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  parse_result (parse_result0)\n",
            " Call ID: parse_result0\n",
            "  Args:\n",
            "    score: 30\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: parse_result\n",
            "\n",
            "{\"score\": 30, \"pass_threshold\": 18, \"passed\": true, \"cum_laude\": false, \"acceptedByStudent\": true}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yes, you will pass the FM 2025 exam. The expected score is 30, which is above the pass threshold of 18.\n"
          ]
        }
      ],
      "source": [
        "response = agent_executor.invoke({\"messages\": {\"role\": \"user\", \"content\": query}}, config = {\"configurable\": {\"thread_id\": \"101\"}})\n",
        "\n",
        "# Print the response\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677edcda",
      "metadata": {
        "id": "677edcda"
      },
      "source": [
        "# Conversetional agents (i.e., chat bot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "947ec79e",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "947ec79e",
        "outputId": "8cb4fc8a-caa9-422a-dc40-805b84512cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "63ea2a84",
      "metadata": {
        "id": "63ea2a84"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
        "from langchain_core.runnables import RunnableMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ad4b11d8",
      "metadata": {
        "id": "ad4b11d8"
      },
      "outputs": [],
      "source": [
        "store = {}\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    \"\"\"Retrieve or create chat history for a session.\"\"\"\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio"
      ],
      "metadata": {
        "id": "7v_nCtpAMOdy"
      },
      "id": "7v_nCtpAMOdy"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "QbTg_AKMiGro"
      },
      "id": "QbTg_AKMiGro",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "])\n",
        "\n",
        "# Chain the prompt with the LLM\n",
        "chain = prompt | llm_model\n",
        "\n",
        "# Create the chatbot with history support\n",
        "chatbot = RunnableWithMessageHistory(chain, get_session_history=get_session_history, input_messages_key=\"input\", history_messages_key=\"history\")"
      ],
      "metadata": {
        "id": "DPFkljz7M4EB"
      },
      "id": "DPFkljz7M4EB",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "def get_response(user_message, history):\n",
        "\n",
        "    # Execute the model\n",
        "    response = chatbot.invoke(input = {\"input\": user_message}, config={\"configurable\": {\"session_id\": session_id}})\n",
        "\n",
        "    # Get the text\n",
        "    generated_text = response.content\n",
        "\n",
        "    # Clear text\n",
        "    matches = list(re.finditer(user_message, generated_text))\n",
        "    if matches:\n",
        "        last_match = matches[-1]\n",
        "        generated_text = generated_text[last_match.end() + 1 :]\n",
        "\n",
        "    return generated_text.strip()\n",
        "\n",
        "# Define the interface\n",
        "session_id = 'user1'\n",
        "interface = gr.ChatInterface(fn=get_response, type=\"messages\", title=f\"ChatBot\", examples=[\"Tell me a joke\", \"What's the capital of France?\", \"What is the population of Trento?\"])\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch(share = True, debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "WpDoJIU1Ko7Q",
        "outputId": "a3182122-9289-4c4f-e1a4-ca7fb12e6c61"
      },
      "id": "WpDoJIU1Ko7Q",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3d1700d0428ed0cef4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3d1700d0428ed0cef4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3d1700d0428ed0cef4.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}