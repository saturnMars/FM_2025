{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoNbujAC+nr/wDUtToRuvs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saturnMars/FM_2025/blob/main/Lab1_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "import pandas as pd\n",
        "import tarfile"
      ],
      "metadata": {
        "id": "2fuw4oEfeNr1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the labelled datasets for:\n",
        "- ***binary* classification**:\n",
        "    1. **Truthfulness** (True/false claims)\n",
        "        - *[The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets](https://github.com/saprmarks/geometry-of-truth/tree/main)*\n",
        "    2. **Subjectivity** (subjective/objetive sentences)\n",
        "        - [CLEF 2025, Task 1 - Subjectivity](https://checkthat.gitlab.io/clef2025/task1/)\n",
        "- ***multiclass* classification**:\n",
        "    3. **Tense** (past/present/future)\n",
        "        - [EnglishTense: A large scale English texts dataset categorized into three categories: Past, Present, Future tenses.](https://data.mendeley.com/datasets/jnb2xp9m4r/2)\n",
        "    4. **Language** (utterances from multiple languages)\n",
        "        - [MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages](https://github.com/alexa/massive)\n",
        "\n"
      ],
      "metadata": {
        "id": "vxIC-kNdef2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.mendeley.com/public-files/datasets/jnb2xp9m4r/files/8148432a-a69a-473f-beb6-835d2a176f30/file_downloaded"
      ],
      "metadata": {
        "id": "3ZYS80qW5T_n",
        "outputId": "aac70621-a027-4480-fdd3-7a372cca2e38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-25 09:44:58--  https://data.mendeley.com/public-files/datasets/jnb2xp9m4r/files/8148432a-a69a-473f-beb6-835d2a176f30/file_downloaded\n",
            "Resolving data.mendeley.com (data.mendeley.com)... 162.159.130.86, 162.159.133.86\n",
            "Connecting to data.mendeley.com (data.mendeley.com)|162.159.130.86|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/28304dc7-a47c-4d83-bdcc-2edc535236d8 [following]\n",
            "--2025-09-25 09:44:59--  https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/28304dc7-a47c-4d83-bdcc-2edc535236d8\n",
            "Resolving prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com)... 3.5.71.180, 52.92.36.218, 52.218.106.104, ...\n",
            "Connecting to prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com)|3.5.71.180|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 458307 (448K) [application/vnd.openxmlformats-officedocument.spreadsheetml.sheet]\n",
            "Saving to: ‘file_downloaded.1’\n",
            "\n",
            "file_downloaded.1   100%[===================>] 447.57K  1.30MB/s    in 0.3s    \n",
            "\n",
            "2025-09-25 09:45:00 (1.30 MB/s) - ‘file_downloaded.1’ saved [458307/458307]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNlcyFegdk4j",
        "outputId": "331295bd-1b54-4162-adce-880879a28f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-25 09:45:03--  https://amazon-massive-nlu-dataset.s3.amazonaws.com/amazon-massive-dataset-1.1.tar.gz\n",
            "Resolving amazon-massive-nlu-dataset.s3.amazonaws.com (amazon-massive-nlu-dataset.s3.amazonaws.com)... 54.231.224.129, 16.182.96.33, 3.5.16.35, ...\n",
            "Connecting to amazon-massive-nlu-dataset.s3.amazonaws.com (amazon-massive-nlu-dataset.s3.amazonaws.com)|54.231.224.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40251390 (38M) [application/x-gzip]\n",
            "Saving to: ‘amazon-massive-dataset-1.1.tar.gz.1’\n",
            "\n",
            "amazon-massive-data 100%[===================>]  38.39M   105MB/s    in 0.4s    \n",
            "\n",
            "2025-09-25 09:45:03 (105 MB/s) - ‘amazon-massive-dataset-1.1.tar.gz.1’ saved [40251390/40251390]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# (1) TRUTHFULNESS (The Geometry of Truth; TRUE|FALSE)\n",
        "truthfulness_df = pd.read_csv(\"https://raw.githubusercontent.com/saprmarks/geometry-of-truth/refs/heads/main/datasets/counterfact_true_false.csv\")\n",
        "truthfulness_df = truthfulness_df[['statement', 'label']].rename(columns = {'statement':'doc'})\n",
        "\n",
        "# (2) SUBJECTIVITY (CLEF2025; SUB|OBJ)\n",
        "subjectivity_df = pd.concat([\n",
        "    pd.read_csv(\"https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task1/data/english/train_en.tsv\", sep= '\\t'),\n",
        "    pd.read_csv(\"https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task1/data/english/dev_en.tsv\", sep= '\\t'),\n",
        "    pd.read_csv(\"https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task1/data/english/dev_test_en.tsv\", sep= '\\t'),\n",
        "    pd.read_csv(\"https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task1/data/english/test_en_labeled.tsv\", sep= '\\t'),\n",
        "])\n",
        "subjectivity_df = subjectivity_df[['sentence', 'label']].rename(columns = {'sentence':'doc'})\n",
        "\n",
        "# (3) TENSE (EnglishTense; past|present|future)\n",
        "tense_df = pd.read_excel(\"https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/28304dc7-a47c-4d83-bdcc-2edc535236d8\").rename(columns = {'Sentence':'doc', 'Labels':'label'})\n",
        "tense_df['label'] = tense_df['label'].str.upper() # Turnaround to fix a bug in the dataset labels\n",
        "\n",
        "# (4) LANGUAGE (MASSIVE; EN/IT/DE/ES)\n",
        "!wget https://amazon-massive-nlu-dataset.s3.amazonaws.com/amazon-massive-dataset-1.1.tar.gz\n",
        "dfs = []\n",
        "with tarfile.open(\"amazon-massive-dataset-1.1.tar.gz\", \"r:gz\") as tar:\n",
        "    for lang in ['en-US', 'it-IT', 'de-DE', 'es-ES']:\n",
        "      dfs.append(pd.read_json(tar.extractfile(path.join('1.1','data', f'{lang}.jsonl')), lines = True))\n",
        "language_df = pd.concat(dfs)[['utt', 'locale']].rename(columns = {'utt':'doc', 'locale': 'label'})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data exploration"
      ],
      "metadata": {
        "id": "6e2X9PM43AQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRUTHFULNESS dataset\n",
        "print('-' * 30, 'TRUTHFULNESS', '-' * 30)\n",
        "print(f\"CLASSES ({truthfulness_df['label'].nunique()}):\", '|'.join(truthfulness_df['label'].map(str).unique()), '\\n')\n",
        "print(truthfulness_df)\n",
        "\n",
        "# SUBJECTIVITY dataset\n",
        "print('-' * 30, 'SUBJECTIVITY', '-' * 30)\n",
        "print(f\"CLASSES ({subjectivity_df['label'].nunique()}):\", '|'.join(subjectivity_df['label'].unique()), '\\n')\n",
        "print(subjectivity_df)\n",
        "\n",
        "# TENSE dataset\n",
        "print('-' * 30, 'TENSE', '-' * 30)\n",
        "print(f\"CLASSES ({tense_df['label'].nunique()}):\", '|'.join(tense_df['label'].unique()), '\\n')\n",
        "print(tense_df)\n",
        "\n",
        "# LANGUAGE dataset\n",
        "print('-' * 30, 'LANGUAGE', '-' * 30)\n",
        "print(f\"CLASSES ({language_df['label'].nunique()}):\", '|'.join(language_df['label'].unique()), '\\n')\n",
        "print(language_df)"
      ],
      "metadata": {
        "id": "aJLer5yI2gI8",
        "outputId": "e9870f32-656e-4d8b-8014-931c37b1d0ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------ TRUTHFULNESS ------------------------------\n",
            "CLASSES (2): 1|0 \n",
            "\n",
            "                                                     doc  label\n",
            "0      The mother tongue of Danielle Darrieux is French.      1\n",
            "1      The mother tongue of Danielle Darrieux is Engl...      0\n",
            "2      The official religion of Edwin of Northumbria ...      1\n",
            "3      The official religion of Edwin of Northumbria ...      0\n",
            "4      The mother tongue of Thomas Joannes Stieltjes ...      1\n",
            "...                                                  ...    ...\n",
            "31959          Jerusalem of Gold was written in Finnish.      0\n",
            "31960  The language used by Jean-Pierre Dionnet is Fr...      1\n",
            "31961  The language used by Jean-Pierre Dionnet is Sp...      0\n",
            "31962                             Subair works as actor.      1\n",
            "31963                          Subair works as composer.      0\n",
            "\n",
            "[31964 rows x 2 columns]\n",
            "------------------------------ SUBJECTIVITY ------------------------------\n",
            "CLASSES (2): SUBJ|OBJ \n",
            "\n",
            "                                                   doc label\n",
            "0    Gone are the days when they led the world in r...  SUBJ\n",
            "1    The trend is expected to reverse as soon as ne...   OBJ\n",
            "2               But there is the specious point again.   OBJ\n",
            "3    He added he wouldn’t be surprised to see a new...   OBJ\n",
            "4    Not less government, you see; the same amount ...  SUBJ\n",
            "..                                                 ...   ...\n",
            "295       She’s also pretty good at winning elections.  SUBJ\n",
            "296  Is the chancellor discombobulated by the anger...  SUBJ\n",
            "297  Realising what my noise habit was doing to me ...  SUBJ\n",
            "298  There are fierce arguments about whether Heath...   OBJ\n",
            "299  None of us are strangers to the negative effec...  SUBJ\n",
            "\n",
            "[2076 rows x 2 columns]\n",
            "------------------------------ TENSE ------------------------------\n",
            "CLASSES (3): FUTURE|PRESENT|PAST \n",
            "\n",
            "                                                     doc    label\n",
            "0      by 2050 ai architects will have designed selfc...   FUTURE\n",
            "1      in the future sustainable transportation optio...   FUTURE\n",
            "2      china has been actively involved in peacekeepi...  PRESENT\n",
            "3      educational diversity is a hallmark of foreign...  PRESENT\n",
            "4        the coach substituted an underperforming player     PAST\n",
            "...                                                  ...      ...\n",
            "13311                       he will become a good person   FUTURE\n",
            "13312                      their door opens after eleven  PRESENT\n",
            "13313                 my mother will cook delicious food   FUTURE\n",
            "13314                        i am going to win this race   FUTURE\n",
            "13315              dona will buy a new mobile next month   FUTURE\n",
            "\n",
            "[13316 rows x 2 columns]\n",
            "------------------------------ LANGUAGE ------------------------------\n",
            "CLASSES (4): en-US|it-IT|de-DE|es-ES \n",
            "\n",
            "                                            doc  label\n",
            "0               wake me up at five am this week  en-US\n",
            "1               wake me up at nine am on friday  en-US\n",
            "2           set an alarm for two hours from now  en-US\n",
            "3                                         quiet  en-US\n",
            "4                                    olly quiet  en-US\n",
            "...                                         ...    ...\n",
            "16516                tengo correos electrónicos  es-ES\n",
            "16517           hay correos electrónicos nuevos  es-ES\n",
            "16518             tengo un nuevo correo de juan  es-ES\n",
            "16519  me ha enviado juan un correo electrónico  es-ES\n",
            "16520         revisa correo electrónico de juan  es-ES\n",
            "\n",
            "[66084 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create *Dataset*, *DataLoader* (PyTorch), and *DataModule* (PyTorch Lightining) for training\n",
        "1. PyTorch: [Dataset/DataLoader](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "2. PyTorch Lightining [DataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html)\n"
      ],
      "metadata": {
        "id": "BcVEVSY3II1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch"
      ],
      "metadata": {
        "id": "WeNuoMpzIQR3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, df:pd.DataFrame):\n",
        "\n",
        "        # Create our inputs (Xs) and target outputs (Ys)\n",
        "        self.x = df['doc'].values\n",
        "        self.y = df['label'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "YckxFu1TJT9C"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "id": "niqD_TwlOr8k",
        "outputId": "ee011da5-c377-430a-da0b-05404d139617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightning in /usr/local/lib/python3.12/dist-packages (2.5.5)\n",
            "Requirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.12/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.3.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (0.15.2)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (25.0)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (1.8.2)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.15.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (from lightning) (2.5.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.12.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics<3.0,>0.7.0->lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.20.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning import LightningDataModule"
      ],
      "metadata": {
        "id": "yqRBoxA4L5Ko"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataModule(LightningDataModule):\n",
        "    def __init__(self, data: Dataset, batch_size: int = 32, val_size:float = 0.1, test_size:float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize the variables\n",
        "        self.data = data\n",
        "\n",
        "        self.train_size = 1 - val_size - test_size\n",
        "        self.val_size = val_size\n",
        "        self.test_size = test_size\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Set the seed for reproducibility\n",
        "        self.random_seed = 101\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        print('STAGE:', stage)\n",
        "\n",
        "        # Create the splits\n",
        "        train_set, val_set, test_set = random_split(\n",
        "            dataset = self.data,\n",
        "            generator = torch.Generator().manual_seed(self.random_seed),\n",
        "            lengths = [self.train_size, self.val_size, self.test_size])\n",
        "\n",
        "        self.train_set = train_set\n",
        "        self.val_set = val_set\n",
        "        self.test_set = test_set\n",
        "\n",
        "        print('\\nINPUTS:', len(self.data), '--> TRAIN:', round(((len(self.train_set) / len(self.data)) * 100), 1), '%',\n",
        "              '|| VALIDATION:', round(((len(self.val_set) / len(self.data)) * 100), 1), '%',\n",
        "              '|| TEST:', round(((len(self.test_set) / len(self.data)) * 100), 1), '%', '\\n')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_set, batch_size = self.batch_size, shuffle = True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_set, batch_size = self.batch_size, shuffle = False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_set, batch_size = self.batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "ilFDZG5VLqFN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the dataloader"
      ],
      "metadata": {
        "id": "FWg138kwP9M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset(tense_df)\n",
        "dataloaders = MyDataModule(dataset, batch_size = 32, val_size = 0.1, test_size = 0.1)\n",
        "\n",
        "#dataloaders.setup()\n",
        "#train_set = dataloaders.train_dataloader()\n",
        "#print(train_set)"
      ],
      "metadata": {
        "id": "xDe9et3IP79G",
        "outputId": "d0276f54-7018-4614-fa44-8ac87beb3b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "MyDataModule.setup() missing 1 required positional argument: 'stage'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4116521620.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MyDataModule.setup() missing 1 required positional argument: 'stage'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create our custom model for classification: a frozen LLM with a Multi Layer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "sGIkgriGR6Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "kU2rFz5kSUzq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, llm_name:str, num_classes: int):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Set the latent dimension\n",
        "        self.latent_dim = 512\n",
        "\n",
        "        # Set the probability for the dropout layer\n",
        "        self.drop_p = 0.3\n",
        "\n",
        "        # Load the foundation language model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
        "        self.llm = AutoModel.from_pretrained(llm_name)\n",
        "\n",
        "        # Create our custom layers\n",
        "        self.decoder_layer = nn.Sequential(\n",
        "\n",
        "            # Layer 0\n",
        "            nn.Linear(self.llm.config.hidden_size, self.latent_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(self.latent_dim),\n",
        "            nn.Dropout(self.drop_p),\n",
        "\n",
        "            # Layer 1\n",
        "            nn.Linear(self.latent_dim, self.latent_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(self.latent_dim),\n",
        "        )\n",
        "\n",
        "        # Create our output layer\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.LayerNorm(self.latent_dim),\n",
        "            nn.Linear(self.latent_dim, self.num_classes),\n",
        "\n",
        "            # Get probability distribution over the classes [batch_size, num_classes]\n",
        "            nn.SoftMax(dim = 1)\n",
        "        )\n",
        "\n",
        "        def forward(self, x):\n",
        "\n",
        "            # Process the document using the frozen LLM\n",
        "            embeddings = self.llm(self.tokenizer(x))\n",
        "\n",
        "            # Learn the latent fetures from the LLM embeddings\n",
        "            out = self.decoder_layer(embeddings)\n",
        "\n",
        "            # Output layer with the SoftMax\n",
        "            out = self.output_layer(out)\n",
        "\n",
        "            return out\n"
      ],
      "metadata": {
        "id": "Sb9kJVzDSOgm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the loss function and the training process"
      ],
      "metadata": {
        "id": "woHOnid7XlfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning import LightningModule"
      ],
      "metadata": {
        "id": "c7w3dQrKXEqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(LightningModule):\n",
        "    def __init__(self, llm_name:str, num_classes: int, configs: dict):\n",
        "         super(Classifier).__init__()\n",
        "\n",
        "         # Unpacked the configs\n",
        "         self.lr = configs['lr']\n",
        "\n",
        "         # Load our custom model\n",
        "         self.model = Network(llm_name, num_classes)\n",
        "\n",
        "         # Define the loss function\n",
        "         self.loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define the optimizer\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr = self.lr)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def _step(self, batch, batch_idx):\n",
        "\n",
        "        # Unpack the batch\n",
        "        x, y = batch\n",
        "\n",
        "        # Forward pass\n",
        "        y_hat = self(x)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = self.loss_function(y_hat, y)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss  = self._step(batch, batch_idx)\n",
        "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        with torch.inference_mode():\n",
        "            loss  = self._step(batch, batch_idx)\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        with torch.inference_mode():\n",
        "            loss  = self._step(batch, batch_idx)\n",
        "        self.log('test_loss', loss)\n"
      ],
      "metadata": {
        "id": "BaxP8SacW_mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train our custom neural models with Pytorch Lighting"
      ],
      "metadata": {
        "id": "4C2y_zDzWlnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Classifier(\n",
        "\n",
        "\n",
        "        input_dim = loader.get_input_dim(),\n",
        "        output_dim = loader.get_target_dim(),\n",
        "        learning_rate = 1e-4,\n",
        "        weight_decay = 1e-4,\n",
        "        drop_p = 0.5)"
      ],
      "metadata": {
        "id": "MX5OzamuWw6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}