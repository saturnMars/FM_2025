{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16943dd",
   "metadata": {},
   "source": [
    "# Overview metrics for text generation\n",
    "## *Lexical-based similarity* (ngrams-based comparison)\n",
    "1. ROUGE: [Recall-Oriented Understudy for Gisting Evaluation](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499)\n",
    "2. BLEU: [BiLingual Evaluation Understudy](https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec25c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [\"The sun set behind the hills\"]\n",
    "candidate = [\"The moon set behind the hills\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7ef3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a2423",
   "metadata": {},
   "source": [
    "### ROUGE: [Recall-Oriented Understudy for Gisting Evaluation](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e538935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The sun set behind the hills\" <<-->> \"The moon set behind the hills\"\n",
      "--> ROUGE-1 F1 Score: 0.83\n",
      "--> ROUGE-2 F1 Score: 0.60\n",
      "--> ROUGE-L F1 Score: 0.83\n"
     ]
    }
   ],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# ROUGE expects plain text inputs\n",
    "rouge_results = rouge_metric.compute(predictions=candidate, references=reference)\n",
    "\n",
    "# Access ROUGE scores (no need for indexing into the result)\n",
    "print(f'\"{reference[0]}\" <<-->> \"{candidate[0]}\"')\n",
    "print(f\"--> ROUGE-1 F1 Score: {rouge_results['rouge1']:.2f}\")\n",
    "print(f\"--> ROUGE-2 F1 Score: {rouge_results['rouge2']:.2f}\")\n",
    "print(f\"--> ROUGE-L F1 Score: {rouge_results['rougeL']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019927e",
   "metadata": {},
   "source": [
    "### BLEU: [BiLingual Evaluation Understudy](https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ca8e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The sun set behind the hills\" <<-->> \"The moon set behind the hills\"\n",
      "--> BLEU Score: 53.73\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# BLEU expects plain text inputs\n",
    "bleu_results = bleu_metric.compute(predictions=candidate, references=reference)\n",
    "print(f'\"{reference[0]}\" <<-->> \"{candidate[0]}\"')\n",
    "print(f\"--> BLEU Score: {bleu_results['bleu'] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fe83b",
   "metadata": {},
   "source": [
    "## Semantic-based similarity (embedding-based comparison)\n",
    "1. [BertSCORE](https://medium.com/@abonia/bertscore-explained-in-5-minutes-0b98553bfb71)\n",
    "2. Cosine similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe60319",
   "metadata": {},
   "source": [
    "### BertSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b327ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The sun set behind the hills\" <<-->> \"The moon set behind the hills\"\n",
      "--> BERTScore (F1): 0.92\n"
     ]
    }
   ],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# BERTScore\n",
    "bert_result = bertscore.compute(predictions=candidate, references=reference, model_type=\"bert-base-uncased\")\n",
    "print(f'\"{reference[0]}\" <<-->> \"{candidate[0]}\"')\n",
    "print(f\"--> BERTScore (F1): {bert_result['f1'][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884166fd",
   "metadata": {},
   "source": [
    "### Cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c097bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edea872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58249acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The sun set behind the hills\" <<-->> \"The moon set behind the hills\"\n",
      "--> Cosine Similarity: 0.71\n"
     ]
    }
   ],
   "source": [
    "referece_embedding = embedding_model.encode(reference)\n",
    "candidate_embedding = embedding_model.encode(candidate)\n",
    "\n",
    "similarity = embedding_model.similarity(referece_embedding, candidate_embedding)\n",
    "print(f'\"{reference[0]}\" <<-->> \"{candidate[0]}\"')\n",
    "print(f\"--> Cosine Similarity: {similarity[0][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd948b9",
   "metadata": {},
   "source": [
    "#### Information retrival based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d4d5ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many people live in London?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94ed1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"London is known for its financial district\",\n",
    "    \"London has 9,787,426 inhabitants at the 2011 census\",\n",
    "    \"The United Kingdom is the fourth largest exporter of goods in the world\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cc2f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Score: 0.75] London has 9,787,426 inhabitants at the 2011 census\n",
      "[Score: 0.46] London is known for its financial district\n",
      "[Score: 0.26] The United Kingdom is the fourth largest exporter of goods in the world\n"
     ]
    }
   ],
   "source": [
    "query_embedding = embedding_model.encode(query)\n",
    "doc_embeddings = embedding_model.encode(docs)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = embedding_model.similarity(query_embedding, doc_embeddings).squeeze()\n",
    "similarities = dict(sorted(zip(docs, similarities), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "for doc, score in similarities.items():\n",
    "    print(f\"[Score: {score.item():.2f}] {doc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
