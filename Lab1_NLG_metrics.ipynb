{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16943dd",
   "metadata": {},
   "source": [
    "# Overview metrics for text generation\n",
    "## *Lexical-based similarity* (ngrams-based comparison)\n",
    "1. ROUGE: [Recall-Oriented Understudy for Gisting Evaluation](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499)\n",
    "2. BLEU: [BiLingual Evaluation Understudy](https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec25c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [\"The sun set behind the hills\"]\n",
    "candidate = [\"The moon set behind the hills\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ef3d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bronzini/repositories/FM_2025/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a2423",
   "metadata": {},
   "source": [
    "### ROUGE: [Recall-Oriented Understudy for Gisting Evaluation](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e538935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The sun set behind the hills\" <<-->> \"The moon set behind the hills\"\n",
      "--> ROUGE-1 F1 Score: 0.83\n",
      "--> ROUGE-2 F1 Score: 0.60\n",
      "--> ROUGE-L F1 Score: 0.83\n"
     ]
    }
   ],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# ROUGE expects plain text inputs\n",
    "rouge_results = rouge_metric.compute(predictions=candidate, references=reference)\n",
    "\n",
    "# Access ROUGE scores (no need for indexing into the result)\n",
    "print(f'\"{reference[0]}\" <<-->> \"{candidate[0]}\"')\n",
    "print(f\"--> ROUGE-1 F1 Score: {rouge_results['rouge1']:.2f}\")\n",
    "print(f\"--> ROUGE-2 F1 Score: {rouge_results['rouge2']:.2f}\")\n",
    "print(f\"--> ROUGE-L F1 Score: {rouge_results['rougeL']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019927e",
   "metadata": {},
   "source": [
    "### BLEU: [BiLingual Evaluation Understudy](https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca8e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The sun set behind the hills\" <<-->> \"The moon set behind the hills\"\n",
      "--> BLEU Score: 53.73\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# BLEU expects plain text inputs\n",
    "bleu_results = bleu_metric.compute(predictions=candidate, references=reference)\n",
    "print(f'\"{reference[0]}\" <<-->> \"{candidate[0]}\"')\n",
    "print(f\"--> BLEU Score: {bleu_results['bleu'] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fe83b",
   "metadata": {},
   "source": [
    "## Semantic-based similarity (embedding-based comparison)\n",
    "1. [BertSCORE](https://medium.com/@abonia/bertscore-explained-in-5-minutes-0b98553bfb71)\n",
    "2. Cosine similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe60319",
   "metadata": {},
   "source": [
    "### BertSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b327ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The sun set behind the hills\" <<-->> \"The moon set behind the hills\"\n",
      "--> BERTScore (F1): 0.92\n"
     ]
    }
   ],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# BERTScore\n",
    "bert_result = bertscore.compute(predictions=candidate, references=reference, model_type=\"bert-base-uncased\")\n",
    "print(f'\"{reference[0]}\" <<-->> \"{candidate[0]}\"')\n",
    "print(f\"--> BERTScore (F1): {bert_result['f1'][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884166fd",
   "metadata": {},
   "source": [
    "### Cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c097bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edea872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58249acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The sun set behind the hills\" <<-->> \"The moon set behind the hills\"\n",
      "--> Cosine Similarity: 0.71\n"
     ]
    }
   ],
   "source": [
    "referece_embedding = embedding_model.encode(reference)\n",
    "candidate_embedding = embedding_model.encode(candidate)\n",
    "\n",
    "similarity = embedding_model.similarity(referece_embedding, candidate_embedding)\n",
    "print(f'\"{reference[0]}\" <<-->> \"{candidate[0]}\"')\n",
    "print(f\"--> Cosine Similarity: {similarity[0][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd948b9",
   "metadata": {},
   "source": [
    "#### Information retrival based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4d5ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many people live in London?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94ed1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"London is known for its financial district\",\n",
    "    \"London has 9,787,426 inhabitants at the 2011 census\",\n",
    "    \"The United Kingdom is the fourth largest exporter of goods in the world\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cc2f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Score: 0.75] London has 9,787,426 inhabitants at the 2011 census\n",
      "[Score: 0.46] London is known for its financial district\n",
      "[Score: 0.26] The United Kingdom is the fourth largest exporter of goods in the world\n"
     ]
    }
   ],
   "source": [
    "query_embedding = embedding_model.encode(query)\n",
    "doc_embeddings = embedding_model.encode(docs)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = embedding_model.similarity(query_embedding, doc_embeddings).squeeze()\n",
    "similarities = dict(sorted(zip(docs, similarities), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "for doc, score in similarities.items():\n",
    "    print(f\"[Score: {score.item():.2f}] {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74cac2c",
   "metadata": {},
   "source": [
    "# Real-world use cases with pre-trained LLMs\n",
    "1. Machine Translation (MT)\n",
    "2. Retrieval-Augmented Generation (RAG)\n",
    "3. Question-answering (QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf310631",
   "metadata": {},
   "source": [
    "### Load the pre-trained LLM\n",
    "* allenai/OLMo-2-0425-1B-Instruct\n",
    "* microsoft/Phi-4-mini-instruct\n",
    "* Qwen/Qwen3-4B-Instruct-2507\n",
    "* HuggingFaceTB/SmolLM3-3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf94a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"allenai/OLMo-2-0425-1B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8df6dc",
   "metadata": {},
   "source": [
    "#### Load the LLM using the pipeline function (a high-level helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d29e8150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task = \"text-generation\", model = model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad89b8",
   "metadata": {},
   "source": [
    "### (1) Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7132adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"You are a professional translator. Your task is to translate the following text into Italian. Provide only the translation, without explanations or additional commentary.\\nTEXT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "871fd20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \"Life is full of surprises.\"\n",
      "Output: \"Vita è piena di sorprese.\"\n",
      "\n",
      "Input: \"They played soccer last weekend.\"\n",
      "Output: \"1953 si giocavano a calcio la settimana precedente.\"\n",
      "\n",
      "Input: \"We are going to the park later.\"\n",
      "Output: \"Vadremo al parco dopo.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = ['Life is full of surprises.', 'They played soccer last weekend.', 'We are going to the park later.']\n",
    "\n",
    "outputs = []\n",
    "for doc in docs:\n",
    "    \n",
    "    # Create the prompt by combining the task description with the document\n",
    "    prompt = f\"{task} {doc}\\nTRANSLATION: \"\n",
    "    \n",
    "    # Generate the output using the pipeline\n",
    "    result = pipe([prompt], return_full_text = False)\n",
    "    generated_text = result[0][0]['generated_text']\n",
    "    \n",
    "    # Clean up the output by removing whitespaces and newlines\n",
    "    generated_text = generated_text.split(\"\\n\")[0].strip()\n",
    "    print(f'Input: \"{doc}\"\\nOutput: \"{generated_text}\"\\n')\n",
    "        \n",
    "    # Store the generated output\n",
    "    outputs.append(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad8a78",
   "metadata": {},
   "source": [
    "#### Evaluate the outputs using the aforementioned metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "430228df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM OUTPUT: \"Vita è piena di sorprese.\" <<-->> TARGET: \"La vita è piena di sorprese.\"\n",
      "--> BLEU Score: 64.32\n",
      "--> BERTScore (F1): 0.96\n",
      "--> Cosine Similarity: 0.97\n",
      "\n",
      "LLM OUTPUT: \"1953 si giocavano a calcio la settimana precedente.\" <<-->> TARGET: \"Hanno giocato a calcio lo scorso fine settimana.\"\n",
      "--> BLEU Score: 0.00\n",
      "--> BERTScore (F1): 0.78\n",
      "--> Cosine Similarity: 0.68\n",
      "\n",
      "LLM OUTPUT: \"Vadremo al parco dopo.\" <<-->> TARGET: \"Andremo al parco più tardi.\"\n",
      "--> BLEU Score: 0.00\n",
      "--> BERTScore (F1): 0.79\n",
      "--> Cosine Similarity: 0.67\n"
     ]
    }
   ],
   "source": [
    "targets = [\"La vita è piena di sorprese.\", \"Hanno giocato a calcio lo scorso fine settimana.\", \"Andremo al parco più tardi.\"]\n",
    "for output, target in zip(outputs, targets):\n",
    "    \n",
    "    print(f'\\nLLM OUTPUT: \"{output}\" <<-->> TARGET: \"{target}\"')\n",
    "    \n",
    "    # Lexical metrics (ngram-based)\n",
    "    bleu_results = bleu_metric.compute(predictions=[output], references=[target])\n",
    "    print(f\"--> BLEU Score: {bleu_results['bleu'] * 100:.2f}\")\n",
    "    \n",
    "    # Semantic metrics (embedding-based)\n",
    "    bert_result = bertscore.compute(predictions=[output], references=[target], model_type=\"bert-base-uncased\")\n",
    "    print(f\"--> BERTScore (F1): {bert_result['f1'][0]:.2f}\")\n",
    "     \n",
    "    cosine_similarity = embedding_model.similarity(embedding_model.encode(target), embedding_model.encode(output)).squeeze()\n",
    "    print(f\"--> Cosine Similarity: {cosine_similarity:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f91f0f",
   "metadata": {},
   "source": [
    "### (2) Retrieval-Augmented Generation (RAG)\n",
    "Using the [Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/) (SQuAD). It evaluates extractive question answering:\n",
    "* questions are generated by crowdworkers over Wikipedia articles;\n",
    "* each answer is a text span within the input context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6ac5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_docs = [\n",
    "    {\n",
    "        \"question\": \"How do asset prices generally move in relation to interest rates?\",\n",
    "        \"answer\": \"inversely\",\n",
    "        \"context\": \"The Fed then raised the Fed funds rate significantly between July 2004 and July 2006. This contributed to an increase in 1-year and 5-year adjustable-rate mortgage (ARM) rates, making ARM interest rate resets more expensive for homeowners. This may have also contributed to the deflating of the housing bubble, as asset prices generally move inversely to interest rates, and it became riskier to speculate in housing. U.S. housing and financial assets dramatically declined in value after the housing bubble burst.\"\n",
    "    },{\n",
    "        \"question\": \"How can climate changes be determined from soil?\",\n",
    "        \"answer\": \"fossil pollen deposits in sediments\",\n",
    "        \"context\": \"Plant responses to climate and other environmental changes can inform our understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates. Estimates of atmospheric CO2 concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants. Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates. Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.\",\n",
    "    },{\n",
    "        \"question\": \"In what year did Miami's government declare bankruptcy?\",\n",
    "        \"answer\": \"2001\",\n",
    "        \"context\": \"According to the U.S. Census Bureau, in 2004, Miami had the third highest incidence of family incomes below the federal poverty line in the United States, making it the third poorest city in the USA, behind only Detroit, Michigan (ranked #1) and El Paso, Texas (ranked #2). Miami is also one of the very few cities where its local government went bankrupt, in 2001. However, since that time, Miami has experienced a revival: in 2008, Miami was ranked as \\\"America's Cleanest City\\\" according to Forbes for its year-round good air quality, vast green spaces, clean drinking water, clean streets and city-wide recycling programs. In a 2009 UBS study of 73 world cities, Miami was ranked as the richest city in the United States (of four U.S. cities included in the survey) and the world's fifth-richest city, in terms of purchasing power.\",\n",
    "    },{\n",
    "        \"question\": \"Which English philosopher wrote Leviathan in 1651?\",\n",
    "        \"answer\": \"Thomas Hobbes\",\n",
    "        \"context\": \"John Locke, one of the most influential Enlightenment thinkers, based his governance philosophy in social contract theory, a subject that permeated Enlightenment political thought. The English philosopher Thomas Hobbes ushered in this new debate with his work Leviathan in 1651. Hobbes also developed some of the fundamentals of European liberal thought: the right of the individual; the natural equality of all men; the artificial character of the political order (which led to the later distinction between civil society and the state); the view that all legitimate political power must be \\\"representative\\\" and based on the consent of the people; and a liberal interpretation of law which leaves people free to do whatever the law does not explicitly forbid.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "911f7039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: \"How do asset prices generally move in relation to interest rates?\"\n",
      "OUTPUT: \"inversely to interest rates\"\n",
      "\n",
      "QUESTION: \"How can climate changes be determined from soil?\"\n",
      "OUTPUT: \"Plant responses to climate and other environmental changes can inform our understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates. Estimates of atmospheric CO2 concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants. Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates. Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.\"\n",
      "\n",
      "QUESTION: \"In what year did Miami's government declare bankruptcy?\"\n",
      "OUTPUT: \"2001\"\n",
      "\n",
      "QUESTION: \"Which English philosopher wrote Leviathan in 1651?\"\n",
      "OUTPUT: \"Leviathan\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = \"Answer the question based on the context provided. Extract the text span from the context.\"\n",
    "\n",
    "outputs = []\n",
    "for doc in squad_docs:\n",
    "    \n",
    "    # Create the prompt by combining the task description with the document\n",
    "    prompt = f\"{task}\\nQUESTION: {doc['question']}\\nCONTEXT: {doc['context']}\\nANSWER:\"\n",
    "    \n",
    "    # Generate the output using the pipeline\n",
    "    result = pipe([prompt], return_full_text = False)\n",
    "    generated_text = result[0][0]['generated_text']\n",
    "    \n",
    "    # Clean up the output by removing whitespaces and newlines\n",
    "    generated_text = generated_text.split(\"\\n\")[0].strip()\n",
    "    print(f'QUESTION: \"{doc[\"question\"]}\"\\nOUTPUT: \"{generated_text}\"\\n')\n",
    "        \n",
    "    # Store the generated output\n",
    "    outputs.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59746771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM OUTPUT: \"inversely to interest rates\" <<-->> TARGET: \"inversely\"\n",
      "--> BLEU Score: 0.00\n",
      "--> Exact Match (EM): False\n",
      "--> Is mentioned: True\n",
      "--> BERTScore (F1): 0.64\n",
      "--> Cosine Similarity: 0.57\n",
      "\n",
      "LLM OUTPUT: \"Plant responses to climate and other environmental changes can inform our understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates. Estimates of atmospheric CO2 concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants. Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates. Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.\" <<-->> TARGET: \"fossil pollen deposits in sediments\"\n",
      "--> BLEU Score: 2.29\n",
      "--> Exact Match (EM): False\n",
      "--> Is mentioned: True\n",
      "--> BERTScore (F1): 0.43\n",
      "--> Cosine Similarity: 0.46\n",
      "\n",
      "LLM OUTPUT: \"2001\" <<-->> TARGET: \"2001\"\n",
      "--> BLEU Score: 0.00\n",
      "--> Exact Match (EM): True\n",
      "--> Is mentioned: True\n",
      "--> BERTScore (F1): 1.00\n",
      "--> Cosine Similarity: 1.00\n",
      "\n",
      "LLM OUTPUT: \"Leviathan\" <<-->> TARGET: \"Thomas Hobbes\"\n",
      "--> BLEU Score: 0.00\n",
      "--> Exact Match (EM): False\n",
      "--> Is mentioned: False\n",
      "--> BERTScore (F1): 0.44\n",
      "--> Cosine Similarity: 0.27\n"
     ]
    }
   ],
   "source": [
    "targets = [doc[\"answer\"] for doc in squad_docs]\n",
    "for output, target in zip(outputs, targets):\n",
    "    \n",
    "    print(f'\\nLLM OUTPUT: \"{output}\" <<-->> TARGET: \"{target}\"')\n",
    "    \n",
    "    # Lexical metrics (ngram-based)\n",
    "    bleu_results = bleu_metric.compute(predictions=[output], references=[target])\n",
    "    print(f\"--> BLEU Score: {bleu_results['bleu'] * 100:.2f}\")\n",
    "    \n",
    "    exact_match = target == output\n",
    "    print('--> Exact Match (EM):', exact_match)\n",
    "    \n",
    "    mentioned = target in output\n",
    "    print('--> Is mentioned:', mentioned)\n",
    "    \n",
    "    # Semantic metrics (embedding-based)\n",
    "    bert_result = bertscore.compute(predictions=[output], references=[target], model_type=\"bert-base-uncased\")\n",
    "    print(f\"--> BERTScore (F1): {bert_result['f1'][0]:.2f}\")\n",
    "     \n",
    "    cosine_similarity = embedding_model.similarity(embedding_model.encode(target), embedding_model.encode(output)).squeeze()\n",
    "    print(f\"--> Cosine Similarity: {cosine_similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7c60a",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "1. Zero-shot (as above)\n",
    "2. In-Context Learning (ICL; include examples in the prompts)\n",
    "3. Chain-of-Thought (CoT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
