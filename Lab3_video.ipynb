{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCRAg8pvIhgV"
      },
      "source": [
        "# In Class Project\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a-yNEwNSeel"
      },
      "source": [
        "## Problem formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8JEWabVRu4G"
      },
      "source": [
        "The aim of this final lab is to give you the possibility to work on a sample project. This will give you a better grasp of how the final project will be conducted and allow you more coding practice compared to previous labs.\n",
        "\n",
        "The sample project is inspired by the paper [Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning](https://arxiv.org/pdf/2203.02053.pdf). During the course, we discussed how Vision-Language Models (VLMs) can align representations from language and vision. This alignment can be achieved using contrastive learning on image/captions pairs. However, the paper shows that performing a dimensionality reduction technique on embedded image/caption pairs results in the two modalities being disjointed in the embedding space. You can better visualize this through the following image:\n",
        "\n",
        "<img src=\"https://modalitygap.readthedocs.io/en/latest/_images/Figure1.png\" width=\"600\">\n",
        "\n",
        "As shown in the image, the \"gap\" between the two modalities is present for a randomly initialized network and persists even after the pretraining phase. Moreover, this modality gap is not only present in images/text pairs but also when text is aligned with other modalities (videos, medical images, amino-acid sequences).\n",
        "\n",
        "Geometrically, the authors talk about a \"cone effect,\" which means that with a growing number of dimensions, embeddings tend to occupy smaller regions of the space assuming a cone-like shape.\n",
        "\n",
        "Your task will consist of testing whether the \"modality gap\" and \"cone effect\" exist using video/caption pairs and a state-of-the-art VLM model, [COCA](https://arxiv.org/pdf/2205.01917.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDcNM7DNR-0U"
      },
      "source": [
        "## Overview of the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeMz3l8TSmpt"
      },
      "source": [
        "The project is divided into three main phases, explained in detail in the following sections. These phases are sequential, and you can perform all of them here on Colab using the available T4 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25CYgoIMSbR4"
      },
      "source": [
        "### Step 1: Video Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L68vMAz-UFVk"
      },
      "source": [
        "Your first task is to perform Video Captioning, which is the automatic captioning of a video by understanding the actions and events in it.\n",
        "\n",
        "We will provide you with short videos and no captions. Then, you'll have to extract a number of frames from each video and generate an independent caption for each frame using COCA.\n",
        "\n",
        "You are free to choose how many frames to extract from each video, how many of these frames will be used to generate captions, how many captions to retain for each video, and which strategy to use to generate captions.\n",
        "\n",
        "As you can imagine, many frames of a single video might be repetitive and lead to the same caption, and conversely, few key frames might contain different actions that are necessary to understand the dynamic of the event represented in the video (For example, think about a tennis player serving; probably the initial frames of the video will depict the player in a static position, focusing and preparing to serve, whereas the act of serving will be present only in fewer frames). There are multiple solutions to this problem, and you are free to choose one. Some examples are:\n",
        "\n",
        "* Subsample only a smaller number of frames and generate captions only for those (faster).\n",
        "* Filter captions based on their diversity. An approach is to cluster similar captions using the text encoder of COCA and take only one caption for each cluster (slower)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WIA938RJFrk"
      },
      "source": [
        "### Step 2: Caption Aggregation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH9B4mWyQqs5"
      },
      "source": [
        "At the end of step 1, you'll have a collection of captions for each single video, with these captions describing only some frames but not the video overall.\n",
        "\n",
        "Your second task is to obtain a single summary describing the content of a video by aggregating the content of your captions. To this end, you have to choose a LLM and prompt it to generate an overall description of a video given a list of captions.\n",
        "\n",
        "As an LLM you can use a model from the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) family. These models have good instruction-following capabilities and work well out-of-the-box. If you have any other preference, you are free to choose other LLMs.\n",
        "\n",
        "P.S. After doing some experiments I found the [Phi-1.5](https://huggingface.co/microsoft/phi-1_5) model to be a good compromise in terms of memory requirements and performance. This model had no training to follow instructions, so to make it do what you want you will have to provide in-context examples and see how it performs in a few shots manner. See image below:\n",
        "\n",
        "<img src=\"https://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image11.gif\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKy5XLZ7JKS5"
      },
      "source": [
        "### Step 3: Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV1seZKYQyTX"
      },
      "source": [
        "Once you have paired videos and captions, you are ready to see if the \"modality gap\" is present for these data points.\n",
        "\n",
        "Your task is now to encode both videos and captions and use U-MAP dimensionality reduction to project the embeddings in a 2-D space. Since videos are composed of multiple frames, you'll have to use a fusion strategy to aggregate the embedding of all frames. For example, you can take the average of the embeddings to represent a whole video.\n",
        "\n",
        "If the results are as expected, at this point, you should see two different clusters of data points, each representing one modality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHENW8jMSPZ2"
      },
      "source": [
        "## Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhaHtAquRC1a"
      },
      "source": [
        "All the tools necessary to perform these tasks were provided during the course of previous labs.\n",
        "\n",
        "You can also find here a list of tools.\n",
        "\n",
        "* **FFMPEG TO EXTRACT FRAMES FROM VIDEOS**\n",
        "\n",
        "  This is the command for extracting frames from a video specifying the frame rate using FFmpeg:\n",
        "\n",
        "  ```\n",
        "  ffmpeg -i input.mp4 -vf fps=1 %04d.png\n",
        "  ```\n",
        "\n",
        "  `%04d.png` is a sequence pattern type used to interpret the output file names by sequencing them with zero-padded sequential numbers, eg. 0001.png, 0002.png, 0003.png, etc.\n",
        "\n",
        "* **OPEN CLIP TO ACCESS THE COCA MODEL**\n",
        "\n",
        "  This is the [link](https://github.com/mlfoundations/open_clip) to the Open CLIP repository.\n",
        "\n",
        "* **HUGGINGFACE'S TRANSFORMERS TO USE FLAN-T5 OR PHI-1.5**\n",
        "\n",
        "  This is the [link](https://huggingface.co/docs/transformers/model_doc/flan-t5) to the FLAN-T5 models documentation on HuggingFace. This is the [link](https://huggingface.co/microsoft/phi-1_5) to Phi-1.5.\n",
        "\n",
        "* **U-MAP FOR DIMENSIONALITY REDUCTION**\n",
        "\n",
        "  This is the official [documentation](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) of the python implementation of U-MAP.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1.0: Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from huggingface_hub import snapshot_download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get raw videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import path, rename, listdir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Videos: 14950\n"
          ]
        }
      ],
      "source": [
        "video_folder = path.join('data', 'videos')\n",
        "if not path.exists(video_folder):\n",
        "    \n",
        "    # Download the files from Hugging Face\n",
        "    snapshot_download(repo_id=\"friedrichor/ActivityNet_Captions\", repo_type=\"dataset\", allow_patterns=[\"*.tar.part-00*\"], local_dir='./data/raw')\n",
        "    \n",
        "    # Combine the parts into a single tar file and extract it\n",
        "    !cat ./data/raw/ActivityNet_Videos.tar.part-* | tar -vxf - -C ./data\n",
        "    \n",
        "    # Rename the extracted folder to a simpler name\n",
        "    rename(path.join('data', 'Activity_Videos'), path.join(video_folder))\n",
        "    \n",
        "    # Delete the parts to save space\n",
        "    !rm -r -f ./data/raw\n",
        "\n",
        "# Print the number of videos downloaded\n",
        "print('Videos:', len(listdir(video_folder)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'video_id': 'v_QOlSCBRmfWY', 'video': 'v_QOlSCBRmfWY.mp4', 'caption': 'A young woman is seen standing in a room and leads into her dancing. The girl dances around the room while the camera captures her movements. She continues dancing around the room and ends by laying on the floor.', 'source': 'ActivityNet_Captions', 'duration': 82.73, 'timestamps': [[0.8300000000000001, 19.86], [17.37, 60.81], [56.26, 79.42]], 'sentences': ['A young woman is seen standing in a room and leads into her dancing.', 'The girl dances around the room while the camera captures her movements.', 'She continues dancing around the room and ends by laying on the floor.']}\n"
          ]
        }
      ],
      "source": [
        "df = load_dataset(\"friedrichor/ActivityNet_Captions\")\n",
        "print(df['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get the video-caption pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_06taCWoMl8y"
      },
      "source": [
        "# Step 1.1: Extract frames from videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dw5c7DWNoma",
        "outputId": "d09794c5-7506-45e0-dd96-891da8c09202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "N6ESuGJx46Bz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "#import gdown\n",
        "import subprocess\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nIfcg0bCN9Fu"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "sub() missing 1 required positional argument: 'string'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m video_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(video)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# video_name\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# create new directory to store frames from a video\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m frames_dir \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_frames_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/video_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(frames_dir):\n\u001b[1;32m     15\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(frames_dir)\n",
            "\u001b[0;31mTypeError\u001b[0m: sub() missing 1 required positional argument: 'string'"
          ]
        }
      ],
      "source": [
        "# define a directory to store video frames and create it\n",
        "video_frames_dir = \"video_frames\"\n",
        "if not os.path.exists(video_frames_dir):\n",
        "    os.makedirs(video_frames_dir)\n",
        "\n",
        "# iterate through types of videos e.g. kitesurfing, bowling...\n",
        "for i, video in enumerate(glob(\"data/videos/*\")):\n",
        "    video_name = os.path.basename(video).split(\".\")[0]\n",
        "\n",
        "    # video_name\n",
        "    \n",
        "    # create new directory to store frames from a video\n",
        "    frames_dir = re.sub(\"video_sample\", video_frames_dir) + f\"/video_{i}\"\n",
        "    if not os.path.exists(frames_dir):\n",
        "        os.makedirs(frames_dir)\n",
        "        \n",
        "    print(f\"Extracting frames from video {video} into folder {frames_dir}\")\n",
        "        \n",
        "    # define ffmpeg command\n",
        "    ffmpeg_command = [\n",
        "        'ffmpeg',\n",
        "        '-i', video,\n",
        "        '-vf', 'fps=1',\n",
        "        f'{frames_dir}/%02d.png'\n",
        "    ]\n",
        "    \n",
        "    # run command\n",
        "    subprocess.call(ffmpeg_command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riGiPVrWMq-H"
      },
      "source": [
        "# Step 1.2: Caption video frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWtNzlr3VHQN"
      },
      "outputs": [],
      "source": [
        "!pip install -q open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcTTNKZ14vFA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "import torch\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c3zouhXMwSi"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "\n",
        "# instantiate model\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "  model_name=\"coca_ViT-L-14\",\n",
        "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\",\n",
        "  device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apeyvg1_VN9o",
        "outputId": "6098d3aa-2ea1-4a0d-f769-6cadecf8b768"
      },
      "outputs": [],
      "source": [
        "captions_dict = {}\n",
        "\n",
        "# iterate through types of videos e.g. kitesurfing, bowling...\n",
        "for video_dir in tqdm(glob(f\"{video_frames_dir}/*\")):\n",
        "    video_type = re.sub(f\"{video_frames_dir}/\", \"\", video_dir)\n",
        "    captions_dict[video_type] = defaultdict(list)\n",
        "    # iterate through single videos\n",
        "    for i, video in enumerate(glob(f\"{video_dir}/*\")):\n",
        "        # iterate through single frames\n",
        "        for image in glob(f\"{video}/*\"):\n",
        "\n",
        "            # preprocess frame\n",
        "            im = Image.open(image).convert(\"RGB\")\n",
        "            im = preprocess(im).unsqueeze(0).to(device)\n",
        "\n",
        "            # generate caption for frame\n",
        "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                generated = model.generate(im)\n",
        "\n",
        "            # add generated caption to dictionary\n",
        "            captions_dict[video_type][f\"video_{i}\"].append(open_clip.decode(generated[0]).replace(\"<start_of_text>\", \"\").replace(\" <end_of_text>\", \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnAHPO6fVsds",
        "outputId": "0c899295-ddbe-4a57-b9e0-b16a4b04fdf6"
      },
      "outputs": [],
      "source": [
        "captions_dict[\"bowling\"][\"video_0\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLZxTcp7MyYt"
      },
      "source": [
        "# Step 2: Use a LLM to generate a single caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwE1h2gCbgAn"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DivnQyM410Q"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlHvu47DM1qS"
      },
      "outputs": [],
      "source": [
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "# initialize llm\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ue9z2eRbn58",
        "outputId": "3f710a50-9482-460a-e2e4-7fd2da0ed4bf"
      },
      "outputs": [],
      "source": [
        "aggregated_captions_dict = defaultdict(lambda : defaultdict(str))\n",
        "\n",
        "# iterate through types of videos e.g. kitesurfing, bowling...\n",
        "for video_type in tqdm(captions_dict):\n",
        "    # iterate through single videos\n",
        "    for video in captions_dict[video_type]:\n",
        "        sentences = \"\"\n",
        "        # iterate through single frame captions and generate a context\n",
        "        for caption in captions_dict[video_type][video]:\n",
        "            sentences += f\"\\n{caption}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Instruction:\n",
        "        Create a summary sentence that aggregate the meaning of all the sentences provided in the context. The sentences in the context are in cronological order. Provide a concise summary using only the information provided in the context.\n",
        "\n",
        "        Context:\n",
        "        The two kids are playing with the cat\n",
        "        The cat is runnning in the living room .\n",
        "        The cat is runnning in the living room .\n",
        "        Two kids are trying to catch a black cat\n",
        "        One child is running in the living room .\n",
        "        The cat is runnning in the living room .\n",
        "        A woman is holding a black cat .\n",
        "        The children and a woman are petting the cat .\n",
        "\n",
        "        Summary:\n",
        "        Two children run into the living room trying to catch a black cat. After a woman catches the cat, they all pet it together .\n",
        "\n",
        "        Context:{sentences}\n",
        "\n",
        "        Summary:\n",
        "        \"\"\"\n",
        "        # generate aggregate captions\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(input_ids, repetition_penalty=1.2, max_new_tokens=50)\n",
        "        result = tokenizer.decode(outputs[0]).split(\"Summary:\")[2].strip().split(\"\\n\")[0]\n",
        "\n",
        "        aggregated_captions_dict[video_type][video] = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr38Pl4-cafT",
        "outputId": "cb77f116-7482-40a2-dd7e-cf8971ec5941"
      },
      "outputs": [],
      "source": [
        "# inspect generated aggregate captions\n",
        "for video_type in aggregated_captions_dict:\n",
        "    print(video_type.upper())\n",
        "    for video in aggregated_captions_dict[video_type]:\n",
        "        print(video.upper())\n",
        "        print(\"ORIGINAL CAPTIONS :\")\n",
        "        for caption in captions_dict[video_type][video]:\n",
        "            print(caption)\n",
        "        print(\"AGGREGATED :\")\n",
        "        print(aggregated_captions_dict[video_type][video])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQN7gL77M2Mk"
      },
      "source": [
        "# Step 3: Project video/caption emebeddings in a low dimensional space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b435gbLgXV3j"
      },
      "outputs": [],
      "source": [
        "!pip install -q umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO5CQohA4hxu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from open_clip.factory import get_tokenizer\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6R_nOlReJgn"
      },
      "outputs": [],
      "source": [
        "# instantiate again COCA\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "  model_name=\"coca_ViT-L-14\",\n",
        "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\",\n",
        "  device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc96Sg06bEft",
        "outputId": "6a314694-7573-4c91-cb20-990e22584649"
      },
      "outputs": [],
      "source": [
        "video_embeddings = []\n",
        "\n",
        "for video_type in aggregated_captions_dict:\n",
        "    for video in range(3):\n",
        "        video_features = []\n",
        "        for frame_path in glob(f\"{video_frames_dir}/{video_type}/video_{i}/*\"):\n",
        "            im = Image.open(frame_path).convert(\"RGB\")\n",
        "            im = preprocess(im).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                image_features = model.encode_image(im).float()\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            video_features.append(image_features.cpu().numpy())\n",
        "        # a video embedding is the average of the frames embeddings\n",
        "        video_embeddings.append(np.mean(np.asarray(video_features), axis=0))\n",
        "\n",
        "video_embeddings = np.asarray(video_embeddings).squeeze()\n",
        "print(video_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xscUbd-nazXn",
        "outputId": "90989dfd-d139-48c1-a0c4-52522e9ccfc5"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"coca_ViT-L-14\")\n",
        "\n",
        "caption_embeddings = []\n",
        "\n",
        "for video_type in aggregated_captions_dict:\n",
        "    for video in range(3):\n",
        "        text_tokens = tokenizer(aggregated_captions_dict[video_type][f\"video_{video}\"])\n",
        "        # the following line fix a bug\n",
        "        text_tokens = text_tokens[:, :torch.where(text_tokens == 0)[1][0] - 1]\n",
        "        with torch.no_grad():\n",
        "            text_features = model.encode_text(text_tokens).float()\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        caption_embeddings.append(text_features.cpu().numpy())\n",
        "\n",
        "caption_embeddings = np.asarray(caption_embeddings).squeeze()\n",
        "print(caption_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "_yRGB83ZXePL",
        "outputId": "7e0c0df8-6326-4804-cdbb-e651bd8856af"
      },
      "outputs": [],
      "source": [
        "# instantiate umap\n",
        "reducer = UMAP()\n",
        "\n",
        "# obtain 2d features\n",
        "features_2d = reducer.fit_transform(np.concatenate([video_embeddings, caption_embeddings], 0))\n",
        "\n",
        "# plot 2d features\n",
        "plt.scatter(features_2d[:-len(video_embeddings), 0], features_2d[:-len(video_embeddings), 1], c='tab:blue', label=\"video\")\n",
        "plt.scatter(features_2d[-len(video_embeddings):, 0], features_2d[-len(video_embeddings):, 1], c='tab:red', label=\"text\")\n",
        "# plot lines\n",
        "for i in range(len(video_embeddings)):\n",
        "    plt.plot([features_2d[i, 0], features_2d[len(video_embeddings)+i, 0]], [features_2d[i, 1], features_2d[len(video_embeddings)+i, 1]], c='black', alpha=0.1)\n",
        "\n",
        "plt.xlabel('umap 1')\n",
        "plt.ylabel('umap 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "Z2wULIw9RTH2",
        "outputId": "3702c2cd-a98b-49fb-a88b-b3b92b2466b7"
      },
      "outputs": [],
      "source": [
        "# define svd\n",
        "def svd(X, n_components=2):\n",
        "    U, S, Vt = np.linalg.svd(X)\n",
        "    return U[:, :n_components] * S[:n_components]\n",
        "\n",
        "# obtain 2d features\n",
        "features_2d = svd(np.concatenate([video_embeddings, caption_embeddings], 0))\n",
        "\n",
        "# plot 2d features\n",
        "plt.scatter(features_2d[:-len(video_embeddings), 0], features_2d[:-len(video_embeddings), 1], c='tab:blue', label=\"video\")\n",
        "plt.scatter(features_2d[-len(video_embeddings):, 0], features_2d[-len(video_embeddings):, 1], c='tab:red', label=\"text\")\n",
        "# plot lines\n",
        "for i in range(len(video_embeddings)):\n",
        "    plt.plot([features_2d[i, 0], features_2d[len(video_embeddings)+i, 0]], [features_2d[i, 1], features_2d[len(video_embeddings)+i, 1]], c='black', alpha=0.1)\n",
        "\n",
        "plt.xlabel('svd 1')\n",
        "plt.ylabel('svd 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
