{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCRAg8pvIhgV"
      },
      "source": [
        "# In Class Project\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a-yNEwNSeel"
      },
      "source": [
        "## Problem formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8JEWabVRu4G"
      },
      "source": [
        "The aim of this final lab is to give you the possibility to work on a sample project. This will give you a better grasp of how the final project will be conducted and allow you more coding practice compared to previous labs.\n",
        "\n",
        "The sample project is inspired by the paper [Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning](https://arxiv.org/pdf/2203.02053.pdf). During the course, we discussed how Vision-Language Models (VLMs) can align representations from language and vision. This alignment can be achieved using contrastive learning on image/captions pairs. However, the paper shows that performing a dimensionality reduction technique on embedded image/caption pairs results in the two modalities being disjointed in the embedding space. You can better visualize this through the following image:\n",
        "\n",
        "<img src=\"https://modalitygap.readthedocs.io/en/latest/_images/Figure1.png\" width=\"600\">\n",
        "\n",
        "As shown in the image, the \"gap\" between the two modalities is present for a randomly initialized network and persists even after the pretraining phase. Moreover, this modality gap is not only present in images/text pairs but also when text is aligned with other modalities (videos, medical images, amino-acid sequences).\n",
        "\n",
        "Geometrically, the authors talk about a \"cone effect,\" which means that with a growing number of dimensions, embeddings tend to occupy smaller regions of the space assuming a cone-like shape.\n",
        "\n",
        "Your task will consist of testing whether the \"modality gap\" and \"cone effect\" exist using video/caption pairs and a state-of-the-art VLM model, [COCA](https://arxiv.org/pdf/2205.01917.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDcNM7DNR-0U"
      },
      "source": [
        "## Overview of the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeMz3l8TSmpt"
      },
      "source": [
        "The project is divided into three main phases, explained in detail in the following sections. These phases are sequential, and you can perform all of them here on Colab using the available T4 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25CYgoIMSbR4"
      },
      "source": [
        "### Step 1: Video Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L68vMAz-UFVk"
      },
      "source": [
        "Your first task is to perform Video Captioning, which is the automatic captioning of a video by understanding the actions and events in it.\n",
        "\n",
        "We will provide you with short videos and no captions. Then, you'll have to extract a number of frames from each video and generate an independent caption for each frame using COCA.\n",
        "\n",
        "You are free to choose how many frames to extract from each video, how many of these frames will be used to generate captions, how many captions to retain for each video, and which strategy to use to generate captions.\n",
        "\n",
        "As you can imagine, many frames of a single video might be repetitive and lead to the same caption, and conversely, few key frames might contain different actions that are necessary to understand the dynamic of the event represented in the video (For example, think about a tennis player serving; probably the initial frames of the video will depict the player in a static position, focusing and preparing to serve, whereas the act of serving will be present only in fewer frames). There are multiple solutions to this problem, and you are free to choose one. Some examples are:\n",
        "\n",
        "* Subsample only a smaller number of frames and generate captions only for those (faster).\n",
        "* Filter captions based on their diversity. An approach is to cluster similar captions using the text encoder of COCA and take only one caption for each cluster (slower)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WIA938RJFrk"
      },
      "source": [
        "### Step 2: Caption Aggregation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH9B4mWyQqs5"
      },
      "source": [
        "At the end of step 1, you'll have a collection of captions for each single video, with these captions describing only some frames but not the video overall.\n",
        "\n",
        "Your second task is to obtain a single summary describing the content of a video by aggregating the content of your captions. To this end, you have to choose a LLM and prompt it to generate an overall description of a video given a list of captions.\n",
        "\n",
        "As an LLM you can use a model from the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) family. These models have good instruction-following capabilities and work well out-of-the-box. If you have any other preference, you are free to choose other LLMs.\n",
        "\n",
        "P.S. After doing some experiments I found the [Phi-1.5](https://huggingface.co/microsoft/phi-1_5) model to be a good compromise in terms of memory requirements and performance. This model had no training to follow instructions, so to make it do what you want you will have to provide in-context examples and see how it performs in a few shots manner. See image below:\n",
        "\n",
        "<img src=\"https://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image11.gif\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp6XCp6RIkwQ"
      },
      "source": [
        "### Step 3: Evaluation with NLG metrics\n",
        "1. *Lexical-based comparsion*: ROUGE\n",
        "2. *Semantic-based comparison*: Cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKy5XLZ7JKS5"
      },
      "source": [
        "### Step 4: Multi-modal alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV1seZKYQyTX"
      },
      "source": [
        "Once you have paired videos and captions, you are ready to see if the \"modality gap\" is present for these data points.\n",
        "\n",
        "Your task is now to encode both videos and captions and use U-MAP dimensionality reduction to project the embeddings in a 2-D space. Since videos are composed of multiple frames, you'll have to use a fusion strategy to aggregate the embedding of all frames. For example, you can take the average of the embeddings to represent a whole video.\n",
        "\n",
        "If the results are as expected, at this point, you should see two different clusters of data points, each representing one modality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3thgHov0IkwQ"
      },
      "source": [
        "### Step 5: Comparison with a native Vision-Language Model (VLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHENW8jMSPZ2"
      },
      "source": [
        "## Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhaHtAquRC1a"
      },
      "source": [
        "All the tools necessary to perform these tasks were provided during the course of previous labs.\n",
        "\n",
        "You can also find here a list of tools.\n",
        "\n",
        "* **FFMPEG TO EXTRACT FRAMES FROM VIDEOS**\n",
        "\n",
        "  This is the command for extracting frames from a video specifying the frame rate using FFmpeg:\n",
        "\n",
        "  ```\n",
        "  ffmpeg -i input.mp4 -vf fps=1 %04d.png\n",
        "  ```\n",
        "\n",
        "  `%04d.png` is a sequence pattern type used to interpret the output file names by sequencing them with zero-padded sequential numbers, eg. 0001.png, 0002.png, 0003.png, etc.\n",
        "\n",
        "* **OPEN CLIP TO ACCESS THE COCA MODEL**\n",
        "\n",
        "  This is the [link](https://github.com/mlfoundations/open_clip) to the Open CLIP repository.\n",
        "\n",
        "* **U-MAP FOR DIMENSIONALITY REDUCTION**\n",
        "\n",
        "  This is the official [documentation](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) of the python implementation of U-MAP.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsIyrVWnIkwR"
      },
      "source": [
        "# Step 1.0: Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nf50ajLtIkwR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from huggingface_hub import snapshot_download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT2Ywl1hIkwS"
      },
      "source": [
        "### Get the video-caption pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qCvE6tDIkwS",
        "outputId": "bd1e6639-c45d-4399-c959-9bbbd483ee36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples: 10009 || ITEM 0: {'video_id': 'v_QOlSCBRmfWY', 'video': 'v_QOlSCBRmfWY.mp4', 'caption': 'A young woman is seen standing in a room and leads into her dancing. The girl dances around the room while the camera captures her movements. She continues dancing around the room and ends by laying on the floor.', 'source': 'ActivityNet_Captions', 'duration': 82.73, 'timestamps': [[0.8300000000000001, 19.86], [17.37, 60.81], [56.26, 79.42]], 'sentences': ['A young woman is seen standing in a room and leads into her dancing.', 'The girl dances around the room while the camera captures her movements.', 'She continues dancing around the room and ends by laying on the floor.']}\n"
          ]
        }
      ],
      "source": [
        "# Load dataset (videos with captions)\n",
        "dataset = load_dataset(\"friedrichor/ActivityNet_Captions\")\n",
        "\n",
        "# Keep only a sample of 1000 examples for testing\n",
        "df = dataset['train']#.take(10)\n",
        "\n",
        "# Visualize first sample\n",
        "print(f\"Samples: {len(df)} || ITEM 0:\", df[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6_MbTISIkwS"
      },
      "source": [
        "### Get raw videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SakRREiAIkwS"
      },
      "outputs": [],
      "source": [
        "from os import path, rename, listdir, remove, makedirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR-RbEtdIkwS",
        "outputId": "bb8db51d-da5c-4a45-d454-28d6f2345798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw videos: 10\n"
          ]
        }
      ],
      "source": [
        "max_num_videos = 10\n",
        "\n",
        "video_folder = path.join('data', 'videos')\n",
        "if not path.exists(video_folder):\n",
        "\n",
        "    # Download the files from Hugging Face --> Take only one part to reduce download size\n",
        "    snapshot_download(repo_id=\"friedrichor/ActivityNet_Captions\", repo_type=\"dataset\", allow_patterns=[\"*.tar.part-000\"], local_dir='./data/raw')\n",
        "\n",
        "    # Combine the parts into a single tar file and extract it\n",
        "    !cat ./data/raw/ActivityNet_Videos.tar.part-* | tar -vxf - -C ./data\n",
        "\n",
        "    # Rename the extracted folder to a simpler name\n",
        "    rename(path.join('data', 'Activity_Videos'), video_folder)\n",
        "\n",
        "    # Delete the parts to save space\n",
        "    !rm -r -f ./data/raw\n",
        "\n",
        "    # Keep only a subset of videos for testing\n",
        "    counter = 0\n",
        "    for idk, video_file in enumerate(listdir(video_folder)):\n",
        "        if counter >= max_num_videos:\n",
        "            remove(path.join(video_folder, video_file))\n",
        "        counter += 1\n",
        "\n",
        "# Print the number of videos downloaded\n",
        "print('Raw videos:', len(listdir(video_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_06taCWoMl8y"
      },
      "source": [
        "# Step 1.1: Extract frames from videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dw5c7DWNoma",
        "outputId": "6b925087-5038-4ec7-acf2-d5c484074775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "N6ESuGJx46Bz"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from os import makedirs\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nIfcg0bCN9Fu"
      },
      "outputs": [],
      "source": [
        "# define a directory to store video frames and create it\n",
        "video_frames_dir = path.join(\"data\", \"frames\")\n",
        "if not path.exists(video_frames_dir):\n",
        "    makedirs(video_frames_dir)\n",
        "\n",
        "# iterate through videos\n",
        "for i, video in enumerate(glob(\"data/videos/*\")):\n",
        "    video_name = path.basename(video).split(\".\")[0]\n",
        "\n",
        "    # create new directory to store frames from a video\n",
        "    frames_dir = path.join(video_frames_dir, video_name)\n",
        "    makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "    # define ffmpeg command\n",
        "    ffmpeg_command = [\n",
        "        'ffmpeg', # ffmpeg/bin/ffmpeg\n",
        "        '-i', video,\n",
        "        '-vf', 'fps=1',\n",
        "        f'{frames_dir}/%02d.png'\n",
        "    ]\n",
        "\n",
        "    # run command\n",
        "    subprocess.call(ffmpeg_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riGiPVrWMq-H"
      },
      "source": [
        "# Step 1.2: Caption video frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fWtNzlr3VHQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea41eae-49a2-46e2-f7ff-97fe1eb71999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.23.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.35.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.6.2)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rcTTNKZ14vFA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import transformers\n",
        "import open_clip\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWz7blC_IkwU"
      },
      "source": [
        "### OpenCLIP, an open source implementation of OpenAI's CLIP (Contrastive Language-Image Pre-training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1c3zouhXMwSi"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "# Instantiate CLIP-inspired model --> Contrastive Captioners (CoCa), Image-Text Foundation Models\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "  model_name=\"coca_ViT-L-14\",\n",
        "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\",\n",
        "  device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Apeyvg1_VN9o",
        "outputId": "d4d04e87-a789-4d9f-a2df-f2d400813f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Please install transformers for generate functionality. `pip install transformers`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2218906085.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# generate caption for frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# add generated caption to dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/open_clip/coca_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, image, text, seq_len, max_seq_len, temperature, generation_type, top_p, top_k, pad_token_id, eos_token_id, sot_token_id, num_beams, num_beam_groups, min_seq_len, stopping_criteria, repetition_penalty, fixed_output_length)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# taking many ideas and components from HuggingFace GenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0m_has_transformers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Please install transformers for generate functionality. `pip install transformers`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seq_len must be larger than min_seq_len\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Please install transformers for generate functionality. `pip install transformers`."
          ]
        }
      ],
      "source": [
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# iterate through video directories\n",
        "captions_dict = defaultdict(list)\n",
        "for video_dir in tqdm(glob(f\"{video_frames_dir}/*\")):\n",
        "\n",
        "    # get video name\n",
        "    video_id = path.basename(video_dir)\n",
        "\n",
        "    # iterate through frames in the video directory\n",
        "    frame_files = sorted(glob(f\"{video_dir}/*\"))\n",
        "    for frame_path in frame_files:\n",
        "\n",
        "        # preprocess frame\n",
        "        im = Image.open(frame_path).convert(\"RGB\")\n",
        "        im = preprocess(im).unsqueeze(0).to(device)\n",
        "\n",
        "        # generate caption for frame\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate(im)\n",
        "\n",
        "        # add generated caption to dictionary\n",
        "        caption = open_clip.decode(generated[0]).replace(\"<start_of_text>\", \"\").replace(\"<end_of_text>\", \"\").strip()\n",
        "        captions_dict[video_id].append(caption)\n",
        "\n",
        "# Clean up memory\n",
        "model.cpu()\n",
        "im.cpu()\n",
        "del model, im\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBoPB0UmIkwU"
      },
      "source": [
        "### Display captions for first video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rCMaCmIvIkwU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "nnAHPO6fVsds",
        "outputId": "e11b0116-401a-4eb9-9274-5f9a8b62a356"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-675425180.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display captions for first video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfirst_video_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Select a number of frames to visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_frames_to_viz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Display captions for first video\n",
        "first_video_id = list(captions_dict.keys())[0]\n",
        "\n",
        "# Select a number of frames to visualize\n",
        "num_frames_to_viz = 6\n",
        "generated_captions = captions_dict[first_video_id][:num_frames_to_viz]\n",
        "\n",
        "# Create the main figure\n",
        "ncols = 2\n",
        "nrows = (num_frames_to_viz + 1) // ncols\n",
        "\n",
        "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(6 * ncols, 3 * nrows))\n",
        "\n",
        "fig.suptitle(f\"Captions for video '{first_video_id}'\", fontsize=16, color = 'firebrick')\n",
        "\n",
        "# Iterate through frames and captions and create subplots\n",
        "for idk, ax in enumerate(axes.flatten()):\n",
        "\n",
        "    # Get the caption\n",
        "    caption = generated_captions[idk]\n",
        "\n",
        "    # Load the image from the disk\n",
        "    img = Image.open(path.join(video_frames_dir, first_video_id, f\"{idk+1:02d}.png\"))\n",
        "\n",
        "    # Visualize the image on the notebook\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f'[FRAME {idk + 1}]\\nGENERATED: \"{caption}')\n",
        "    ax.axis('off')\n",
        "\n",
        "# Visualize the figure\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaiF03qXIkwU"
      },
      "source": [
        "# Step 2: Use a LLM to generate a single caption for a video\n",
        "TASK: Aggregate and summarize the frame captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_ruHDTOIkwV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABlgIR8zIkwV"
      },
      "outputs": [],
      "source": [
        "model_name = 'microsoft/Phi-4-mini-instruct'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ue9z2eRbn58"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Iterate through videos\n",
        "aggregated_captions = dict()\n",
        "for video_id in tqdm(captions_dict):\n",
        "\n",
        "    # Aggregate all the frame captions to generate a context\n",
        "    video_sentences = '\\n'.join(captions_dict[video_id])\n",
        "\n",
        "    # Define the instruction prompt with (1) task description, (2) in-context examples, and (3) input sentence.\n",
        "    prompt = f\"\"\"\n",
        "    Instruction:\n",
        "    Create a summary sentence that aggregate the meaning of all the sentences provided in the context. The sentences in the context are in cronological order. Provide a concise summary using only the information provided in the context.\n",
        "\n",
        "    Context:\n",
        "    The two kids are playing with the cat\n",
        "    The cat is runnning in the living room .\n",
        "    The cat is runnning in the living room .\n",
        "    Two kids are trying to catch a black cat\n",
        "    One child is running in the living room .\n",
        "    The cat is runnning in the living room .\n",
        "    A woman is holding a black cat .\n",
        "    The children and a woman are petting the cat .\n",
        "\n",
        "    Summary:\n",
        "    Two children run into the living room trying to catch a black cat. After a woman catches the cat, they all pet it together .\n",
        "\n",
        "    Context:\n",
        "    {video_sentences}\n",
        "\n",
        "    Summary:\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate aggregate captions using the LLM\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        outputs = model.generate(**inputs, repetition_penalty=1.2, max_new_tokens=50)\n",
        "    generated_text = tokenizer.decode(outputs[0][len(inputs.input_ids.squeeze()):], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Attach the summarized caption to the video\n",
        "    aggregated_captions[video_id] = generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnbyAmsVIkwd"
      },
      "source": [
        "#### Visualize the aggregated caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr38Pl4-cafT"
      },
      "outputs": [],
      "source": [
        "video_id = list(aggregated_captions.keys())[0]\n",
        "\n",
        "print(\"AGGREGATED:\", aggregated_captions[video_id])\n",
        "print(f\"ORIGINAL CAPTIONS ({len(glob(path.join('data', 'frames',video_id, '*.png')))} frames || {len(captions_dict[video_id])} captions):\")\n",
        "for frame_id, caption in enumerate(captions_dict[video_id]):\n",
        "    print(f'- FRAME {frame_id + 1}: \"{caption}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XFHumRFIkwd"
      },
      "source": [
        "# Step 3: Evaluation with NLG metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBacZOUQIkwd"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import seaborn as sns\n",
        "rouge_metric = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd9dQjDrIkwe"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxJY76kpIkwe"
      },
      "outputs": [],
      "source": [
        "# Set verbose to True to visualize per-video results\n",
        "verbose = True\n",
        "\n",
        "# Compute metrics for each video\n",
        "coca_stats = defaultdict(list)\n",
        "for video_id, generated_caption in aggregated_captions.items():\n",
        "\n",
        "    # Get the target caption from the dataset\n",
        "    target_caption = df.filter(lambda x: x['video_id'] == video_id)[0]['caption']\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge_results = rouge_metric.compute(predictions = [generated_caption], references = [target_caption])\n",
        "\n",
        "    # Compute embeddings\n",
        "    target_embedding = embedding_model.encode([target_caption])\n",
        "    generated_embedding = embedding_model.encode([generated_caption])\n",
        "    similarity = embedding_model.similarity(target_embedding, generated_embedding).item()\n",
        "\n",
        "    # Store stats\n",
        "    coca_stats['rouge1'].append(rouge_results['rouge1'])\n",
        "    coca_stats['rouge2'].append(rouge_results['rouge2'])\n",
        "    coca_stats['cosine_sim'].append(similarity)\n",
        "\n",
        "    # Visualize results\n",
        "    if verbose:\n",
        "        print('-' * 15, f'VIDEO ID ({video_id})', '-' * 15)\n",
        "        print(f'TARGET:\"{target_caption}\"\\nGENERATED: \"{generated_caption}\"')\n",
        "        print(f\"--> ROUGE-1 (unigrams) --> F1 Score: {rouge_results['rouge1']:.2f}\")\n",
        "        print(f\"--> ROUGE-2 (bigrams)  --> F1 Score: {rouge_results['rouge2']:.2f}\")\n",
        "        print(f'--> COSINE SIMILARITY: {similarity:.2f}\\n')\n",
        "\n",
        "# Print average results\n",
        "sns.boxplot(data=coca_stats, palette=\"Set2\", width=0.5)\n",
        "plt.title(\"Distribution of Evaluation Metrics for Video Captioning\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQN7gL77M2Mk"
      },
      "source": [
        "# Step 4: Visualise multi-modal alignment by projecting video/caption embeddings in a low dimensional space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO5CQohA4hxu"
      },
      "outputs": [],
      "source": [
        "from open_clip.factory import get_tokenizer\n",
        "from umap import UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6R_nOlReJgn"
      },
      "outputs": [],
      "source": [
        "# instantiate again text-caption model\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "  model_name=\"coca_ViT-L-14\",\n",
        "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\",\n",
        "  device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6hVaTy3Ikwf"
      },
      "source": [
        "### Step 3.1: Get the video embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNs9AO4VIkwf"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Iterate through video files\n",
        "video_embeddings = list()\n",
        "for video_path in tqdm(glob(f\"{video_frames_dir}/*\")):\n",
        "\n",
        "    # get video name\n",
        "    video_id = path.basename(video_dir)\n",
        "\n",
        "    # iterate through frames in the video directory\n",
        "    frame_embeddings = list()\n",
        "    for frame_path in sorted(glob(f\"{video_dir}/*\")):\n",
        "\n",
        "        # preprocess frame\n",
        "        im = Image.open(frame_path).convert(\"RGB\")\n",
        "        im = preprocess(im).unsqueeze(0).to(device)\n",
        "\n",
        "        # generate caption for frame\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(im).float()\n",
        "\n",
        "        # Normalize features\n",
        "        image_features /= image_features.norm(dim=-1) # keepdim=True\n",
        "\n",
        "        # Store the frame features\n",
        "        frame_embeddings.append(image_features.cpu().squeeze())\n",
        "\n",
        "    # Save the video embedding as the average of the frame embeddings\n",
        "    video_embeddings.append(torch.stack(frame_embeddings).mean(dim=0))\n",
        "\n",
        "# Transform list to tensor\n",
        "video_embeddings = torch.stack(video_embeddings)\n",
        "print(\"\\nVideo embeddings:\", video_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYldLENdIkwf"
      },
      "source": [
        "### Step 3.2: Get the caption embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PK8c01AIkwf"
      },
      "outputs": [],
      "source": [
        "# Get the textual tokenizer of the model\n",
        "tokenizer = get_tokenizer(\"coca_ViT-L-14\")\n",
        "\n",
        "# Iterate through aggregated captions to get caption embeddings\n",
        "caption_embeddings = list()\n",
        "for video_id, caption in aggregated_captions.items():\n",
        "\n",
        "    # Tokenize the caption\n",
        "    input_ids = tokenizer(caption).to(device)\n",
        "\n",
        "    # Process the tokenized caption\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(input_ids).float()\n",
        "\n",
        "    # Normalize features\n",
        "    text_features /= text_features.norm(dim=-1)\n",
        "\n",
        "    # Store the caption embedding\n",
        "    caption_embeddings.append(text_features.cpu().squeeze())\n",
        "\n",
        "# Transform list to tensor\n",
        "caption_embeddings = torch.stack(caption_embeddings)\n",
        "print(\"Caption embeddings:\", caption_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At3ZqDSQIkwf"
      },
      "source": [
        "### UMAP: Uniform Manifold Approximation and Projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ_dQ7H6Ikwf"
      },
      "outputs": [],
      "source": [
        "# Instantiate umap\n",
        "reducer = UMAP()\n",
        "\n",
        "# obtain 2d features\n",
        "reduced_features = reducer.fit_transform(torch.concat([video_embeddings, caption_embeddings], dim=0))\n",
        "reduced_videoFeatures = reduced_features[:len(video_embeddings)]\n",
        "reduced_textFeatures = reduced_features[len(video_embeddings):]\n",
        "\n",
        "# plot 2d features\n",
        "plt.scatter(x = reduced_videoFeatures[:, 0], y = reduced_videoFeatures[:, 1], c='tab:blue', label=\"video\")\n",
        "plt.scatter(x = reduced_textFeatures[:, 0], y = reduced_textFeatures[:, 1], c='tab:red', label=\"text\")\n",
        "\n",
        "# Plot conneting lines\n",
        "for i in range(len(video_embeddings)):\n",
        "    point_video = (reduced_videoFeatures[i, 0], reduced_textFeatures[i, 0])\n",
        "    point_text = (reduced_videoFeatures[i, 1], reduced_textFeatures[i, 1])\n",
        "    plt.plot(point_video, point_text, c='black', alpha=0.1)\n",
        "\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbBBJ7kAIkwg"
      },
      "source": [
        "### SVD: Singular Value Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iUZoUn-Ikwg"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import svd\n",
        "def compute_svd(X, n_components=2):\n",
        "    U, S, Vt = svd(X)\n",
        "    return U[:, :n_components] * S[:n_components]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2wULIw9RTH2"
      },
      "outputs": [],
      "source": [
        "# obtain 2d features\n",
        "reduced_features = compute_svd(torch.concat([video_embeddings, caption_embeddings], dim=0))\n",
        "reduced_videoFeatures = reduced_features[:len(video_embeddings)]\n",
        "reduced_textFeatures = reduced_features[len(video_embeddings):]\n",
        "\n",
        "# plot 2d features\n",
        "plt.scatter(x = reduced_videoFeatures[:, 0], y = reduced_videoFeatures[:, 1], c='tab:blue', label=\"Video\")\n",
        "plt.scatter(x = reduced_textFeatures[:, 0], y = reduced_textFeatures[:, 1], c='tab:red', label=\"Text\")\n",
        "\n",
        "# Plot conneting lines\n",
        "for i in range(len(video_embeddings)):\n",
        "    point_video = (reduced_videoFeatures[i, 0], reduced_textFeatures[i, 0])\n",
        "    point_text = (reduced_videoFeatures[i, 1], reduced_textFeatures[i, 1])\n",
        "    plt.plot(point_video, point_text, c='black', alpha=0.1)\n",
        "\n",
        "plt.xlabel('SVD 1')\n",
        "plt.ylabel('SVD 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9ciU5lmIkwg"
      },
      "source": [
        "# Step 5: Comparison with a Vision-Language Model (VLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAba-a6NIkwg"
      },
      "outputs": [],
      "source": [
        "!pip install torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n29Nwso_Ikwh"
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGduQEAbIkwh"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"image-text-to-text\", model= model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNhH4XphIkwh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the instruction for the VLM\n",
        "instruction = \"Describe the video content in a concise sentence.\"\n",
        "\n",
        "# Iterate through videos\n",
        "vlm_generated_captions = dict()\n",
        "for video_path in glob(\"data/videos/*\"):\n",
        "\n",
        "    print(\"Video path:\", path.abspath(video_path))\n",
        "\n",
        "    # Prepare message\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"video\", \"video\": path.abspath(video_path)},\n",
        "            {\"type\": \"text\", \"text\": instruction}]\n",
        "        }]\n",
        "\n",
        "    # Generate response\n",
        "    response = pipe(messages)\n",
        "\n",
        "    # Get the generated text\n",
        "    generated_text = response[0]['generated_text'].strip()\n",
        "\n",
        "    print(\"Response:\", response[0]['generated_text'])\n",
        "\n",
        "    # Get video name\n",
        "    video_name = path.basename(video_path).split(\".\")[0]\n",
        "\n",
        "    # Store the generated caption\n",
        "    vlm_generated_captions[video_name] = generated_text\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M57vOa2zIkwh"
      },
      "source": [
        "## Evaluate the generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xEIR0qUIkwh"
      },
      "outputs": [],
      "source": [
        "# Set verbose to True to visualize per-video results\n",
        "verbose = True\n",
        "\n",
        "# Compute metrics for each video\n",
        "vlm_stats = defaultdict(list)\n",
        "for video_id, generated_caption in vlm_generated_captions.items():\n",
        "\n",
        "    # Get the target caption from the dataset\n",
        "    target_caption = df.filter(lambda x: x['video_id'] == video_id)[0]['caption']\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge_results = rouge_metric.compute(predictions = [generated_caption], references = [target_caption])\n",
        "\n",
        "    # Compute embeddings\n",
        "    target_embedding = embedding_model.encode([target_caption])\n",
        "    generated_embedding = embedding_model.encode([generated_caption])\n",
        "    similarity = embedding_model.similarity(target_embedding, generated_embedding).item()\n",
        "\n",
        "    # Store stats\n",
        "    vlm_stats['rouge1'].append(rouge_results['rouge1'])\n",
        "    vlm_stats['rouge2'].append(rouge_results['rouge2'])\n",
        "    vlm_stats['cosine_sim'].append(similarity)\n",
        "\n",
        "    # Visualize results\n",
        "    if verbose:\n",
        "        print('-' * 15, f'VIDEO ID ({video_id})', '-' * 15)\n",
        "        print(f'TARGET:\"{target_caption}\"\\nGENERATED: \"{generated_caption}\"')\n",
        "        print(f\"--> ROUGE-1 (unigrams) --> F1 Score: {rouge_results['rouge1']:.2f}\")\n",
        "        print(f\"--> ROUGE-2 (bigrams)  --> F1 Score: {rouge_results['rouge2']:.2f}\")\n",
        "        print(f'--> COSINE SIMILARITY: {similarity:.2f}\\n')\n",
        "\n",
        "# Print average results\n",
        "sns.boxplot(data=vlm_stats, palette=\"Set2\", width=0.5)\n",
        "plt.title(\"Distribution of Evaluation Metrics for Video Captioning\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}