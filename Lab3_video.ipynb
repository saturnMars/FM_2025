{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCRAg8pvIhgV"
      },
      "source": [
        "# In Class Project\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a-yNEwNSeel"
      },
      "source": [
        "## Problem formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8JEWabVRu4G"
      },
      "source": [
        "The aim of this final lab is to give you the possibility to work on a sample project. This will give you a better grasp of how the final project will be conducted and allow you more coding practice compared to previous labs.\n",
        "\n",
        "The sample project is inspired by the paper [Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning](https://arxiv.org/pdf/2203.02053.pdf). During the course, we discussed how Vision-Language Models (VLMs) can align representations from language and vision. This alignment can be achieved using contrastive learning on image/captions pairs. However, the paper shows that performing a dimensionality reduction technique on embedded image/caption pairs results in the two modalities being disjointed in the embedding space. You can better visualize this through the following image:\n",
        "\n",
        "<img src=\"https://modalitygap.readthedocs.io/en/latest/_images/Figure1.png\" width=\"600\">\n",
        "\n",
        "As shown in the image, the \"gap\" between the two modalities is present for a randomly initialized network and persists even after the pretraining phase. Moreover, this modality gap is not only present in images/text pairs but also when text is aligned with other modalities (videos, medical images, amino-acid sequences).\n",
        "\n",
        "Geometrically, the authors talk about a \"cone effect,\" which means that with a growing number of dimensions, embeddings tend to occupy smaller regions of the space assuming a cone-like shape.\n",
        "\n",
        "Your task will consist of testing whether the \"modality gap\" and \"cone effect\" exist using video/caption pairs and a state-of-the-art VLM model, [COCA](https://arxiv.org/pdf/2205.01917.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDcNM7DNR-0U"
      },
      "source": [
        "## Overview of the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeMz3l8TSmpt"
      },
      "source": [
        "The project is divided into three main phases, explained in detail in the following sections. These phases are sequential, and you can perform all of them here on Colab using the available T4 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25CYgoIMSbR4"
      },
      "source": [
        "### Step 1: Video Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L68vMAz-UFVk"
      },
      "source": [
        "Your first task is to perform Video Captioning, which is the automatic captioning of a video by understanding the actions and events in it.\n",
        "\n",
        "We will provide you with short videos and no captions. Then, you'll have to extract a number of frames from each video and generate an independent caption for each frame using COCA.\n",
        "\n",
        "You are free to choose how many frames to extract from each video, how many of these frames will be used to generate captions, how many captions to retain for each video, and which strategy to use to generate captions.\n",
        "\n",
        "As you can imagine, many frames of a single video might be repetitive and lead to the same caption, and conversely, few key frames might contain different actions that are necessary to understand the dynamic of the event represented in the video (For example, think about a tennis player serving; probably the initial frames of the video will depict the player in a static position, focusing and preparing to serve, whereas the act of serving will be present only in fewer frames). There are multiple solutions to this problem, and you are free to choose one. Some examples are:\n",
        "\n",
        "* Subsample only a smaller number of frames and generate captions only for those (faster).\n",
        "* Filter captions based on their diversity. An approach is to cluster similar captions using the text encoder of COCA and take only one caption for each cluster (slower)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WIA938RJFrk"
      },
      "source": [
        "### Step 2: Caption Aggregation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH9B4mWyQqs5"
      },
      "source": [
        "At the end of step 1, you'll have a collection of captions for each single video, with these captions describing only some frames but not the video overall.\n",
        "\n",
        "Your second task is to obtain a single summary describing the content of a video by aggregating the content of your captions. To this end, you have to choose a LLM and prompt it to generate an overall description of a video given a list of captions.\n",
        "\n",
        "As an LLM you can use a model from the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) family. These models have good instruction-following capabilities and work well out-of-the-box. If you have any other preference, you are free to choose other LLMs.\n",
        "\n",
        "P.S. After doing some experiments I found the [Phi-1.5](https://huggingface.co/microsoft/phi-1_5) model to be a good compromise in terms of memory requirements and performance. This model had no training to follow instructions, so to make it do what you want you will have to provide in-context examples and see how it performs in a few shots manner. See image below:\n",
        "\n",
        "<img src=\"https://ai.stanford.edu/blog/assets/img/posts/2022-08-01-understanding-incontext/images/image11.gif\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKy5XLZ7JKS5"
      },
      "source": [
        "### Step 3: Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV1seZKYQyTX"
      },
      "source": [
        "Once you have paired videos and captions, you are ready to see if the \"modality gap\" is present for these data points.\n",
        "\n",
        "Your task is now to encode both videos and captions and use U-MAP dimensionality reduction to project the embeddings in a 2-D space. Since videos are composed of multiple frames, you'll have to use a fusion strategy to aggregate the embedding of all frames. For example, you can take the average of the embeddings to represent a whole video.\n",
        "\n",
        "If the results are as expected, at this point, you should see two different clusters of data points, each representing one modality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHENW8jMSPZ2"
      },
      "source": [
        "## Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhaHtAquRC1a"
      },
      "source": [
        "All the tools necessary to perform these tasks were provided during the course of previous labs.\n",
        "\n",
        "You can also find here a list of tools.\n",
        "\n",
        "* **FFMPEG TO EXTRACT FRAMES FROM VIDEOS**\n",
        "\n",
        "  This is the command for extracting frames from a video specifying the frame rate using FFmpeg:\n",
        "\n",
        "  ```\n",
        "  ffmpeg -i input.mp4 -vf fps=1 %04d.png\n",
        "  ```\n",
        "\n",
        "  `%04d.png` is a sequence pattern type used to interpret the output file names by sequencing them with zero-padded sequential numbers, eg. 0001.png, 0002.png, 0003.png, etc.\n",
        "\n",
        "* **OPEN CLIP TO ACCESS THE COCA MODEL**\n",
        "\n",
        "  This is the [link](https://github.com/mlfoundations/open_clip) to the Open CLIP repository.\n",
        "\n",
        "* **HUGGINGFACE'S TRANSFORMERS TO USE FLAN-T5 OR PHI-1.5**\n",
        "\n",
        "  This is the [link](https://huggingface.co/docs/transformers/model_doc/flan-t5) to the FLAN-T5 models documentation on HuggingFace. This is the [link](https://huggingface.co/microsoft/phi-1_5) to Phi-1.5.\n",
        "\n",
        "* **U-MAP FOR DIMENSIONALITY REDUCTION**\n",
        "\n",
        "  This is the official [documentation](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) of the python implementation of U-MAP.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1.0: Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from huggingface_hub import snapshot_download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get the video-caption pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset (videos with captions)\n",
        "dataset = load_dataset(\"friedrichor/ActivityNet_Captions\")\n",
        "\n",
        "# Keep only a sample of 1000 examples for testing\n",
        "df = dataset['train'].take(10)\n",
        "\n",
        "# Visualize first sample\n",
        "print(f\"Samples: {len(df)} || ITEM 0:\", df[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get raw videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import path, rename, listdir, remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_folder = path.join('data', 'videos')\n",
        "if not path.exists(video_folder):\n",
        "    \n",
        "    # Download the files from Hugging Face\n",
        "    snapshot_download(repo_id=\"friedrichor/ActivityNet_Captions\", repo_type=\"dataset\", allow_patterns=[\"*.tar.part-00*\"], local_dir='./data/raw') \n",
        "    \n",
        "    # Combine the parts into a single tar file and extract it\n",
        "    !cat ./data/raw/ActivityNet_Videos.tar.part-* | tar -vxf - -C ./data\n",
        "    \n",
        "    # Rename the extracted folder to a simpler name\n",
        "    rename(path.join('data', 'Activity_Videos'), video_folder)\n",
        "    \n",
        "    # Delete the parts to save space\n",
        "    !rm -r -f ./data/raw\n",
        "    \n",
        "    # Keep only the sampled videos \n",
        "    sampled_video_ids = set(item['video_id'] for item in df)\n",
        "    for video_file in listdir(video_folder):\n",
        "        video_id = video_file.split('.')[0]\n",
        "        if video_id not in sampled_video_ids:\n",
        "            remove(path.join(video_folder, video_file))\n",
        "            \n",
        "# Print the number of videos downloaded\n",
        "print('Raw videos:', len(listdir(video_folder)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_06taCWoMl8y"
      },
      "source": [
        "# Step 1.1: Extract frames from videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dw5c7DWNoma",
        "outputId": "d09794c5-7506-45e0-dd96-891da8c09202"
      },
      "outputs": [],
      "source": [
        "!apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6ESuGJx46Bz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "#import gdown\n",
        "import subprocess\n",
        "from glob import glob\n",
        "from os import makedirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIfcg0bCN9Fu"
      },
      "outputs": [],
      "source": [
        "# define a directory to store video frames and create it\n",
        "video_frames_dir = path.join(\"data\", \"frames\")\n",
        "if not os.path.exists(video_frames_dir):\n",
        "    os.makedirs(video_frames_dir)\n",
        "\n",
        "# iterate through videos\n",
        "for i, video in enumerate(glob(\"data/videos/*\")):\n",
        "    video_name = os.path.basename(video).split(\".\")[0]\n",
        "\n",
        "    # create new directory to store frames from a video\n",
        "    frames_dir = path.join(video_frames_dir, video_name)\n",
        "    makedirs(frames_dir, exist_ok=True)\n",
        "        \n",
        "    # define ffmpeg command\n",
        "    ffmpeg_command = [\n",
        "        'ffmpeg/bin/ffmpeg',\n",
        "        '-i', video,\n",
        "        '-vf', 'fps=1',\n",
        "        f'{frames_dir}/%02d.png'\n",
        "    ]\n",
        "    \n",
        "    # run command\n",
        "    subprocess.call(ffmpeg_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riGiPVrWMq-H"
      },
      "source": [
        "# Step 1.2: Caption video frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWtNzlr3VHQN"
      },
      "outputs": [],
      "source": [
        "!pip install -q open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcTTNKZ14vFA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "import torch\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenCLIP, an open source implementation of OpenAI's CLIP (Contrastive Language-Image Pre-training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c3zouhXMwSi"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "# Instantiate CLIP model\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "  model_name=\"coca_ViT-L-14\",\n",
        "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\",\n",
        "  device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apeyvg1_VN9o",
        "outputId": "6098d3aa-2ea1-4a0d-f769-6cadecf8b768"
      },
      "outputs": [],
      "source": [
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# iterate through video directories\n",
        "captions_dict = defaultdict(list)\n",
        "for video_dir in tqdm(glob(f\"{video_frames_dir}/*\")):\n",
        "    \n",
        "    # get video name\n",
        "    video_id = os.path.basename(video_dir)\n",
        "    \n",
        "    # iterate through frames in the video directory\n",
        "    frame_files = sorted(glob(f\"{video_dir}/*\"))\n",
        "    for frame_path in frame_files:\n",
        "        \n",
        "        # preprocess frame\n",
        "        im = Image.open(frame_path).convert(\"RGB\")\n",
        "        im = preprocess(im).unsqueeze(0).to(device)\n",
        "\n",
        "        # generate caption for frame\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate(im)\n",
        "\n",
        "        # add generated caption to dictionary\n",
        "        caption = open_clip.decode(generated[0]).replace(\"<start_of_text>\", \"\").replace(\" <end_of_text>\", \"\").strip()\n",
        "        captions_dict[video_id].append(caption)\n",
        "\n",
        "# Clean up memory\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display captions for first video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnAHPO6fVsds",
        "outputId": "0c899295-ddbe-4a57-b9e0-b16a4b04fdf6"
      },
      "outputs": [],
      "source": [
        "# Display captions for first video\n",
        "first_video_id = list(captions_dict.keys())[0]\n",
        "\n",
        "# Select a number of frames to visualize\n",
        "num_frames_to_viz = 6\n",
        "generated_captions = captions_dict[first_video_id][:num_frames_to_viz]\n",
        "\n",
        "# Create the main figure\n",
        "ncols = 2\n",
        "nrows = (num_frames_to_viz + 1) // ncols\n",
        "\n",
        "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(6 * ncols, 3 * nrows))\n",
        "\n",
        "fig.suptitle(f\"Captions for video '{first_video_id}'\", fontsize=16, color = 'firebrick')\n",
        "\n",
        "# Iterate through frames and captions and create subplots\n",
        "for idk, ax in enumerate(axes.flatten()):\n",
        "    \n",
        "    # Get the caption\n",
        "    caption = generated_captions[idk]\n",
        "    \n",
        "    # Load the image from the disk\n",
        "    img = Image.open(path.join(video_frames_dir, first_video_id, f\"{idk+1:02d}.png\"))\n",
        "    \n",
        "    # Visualize the image on the notebook\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f'[FRAME {idk + 1}]\\nGENERATED: \"{caption}')\n",
        "    ax.axis('off')\n",
        "\n",
        "# Visualize the figure\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Use a LLM to generate a single caption for a video \n",
        "TASK: Aggregate and summarize the frame captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'microsoft/Phi-4-mini-instruct'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ue9z2eRbn58",
        "outputId": "3f710a50-9482-460a-e2e4-7fd2da0ed4bf"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Iterate through videos\n",
        "aggregated_captions = dict()\n",
        "for video_id in tqdm(captions_dict):\n",
        "    \n",
        "    # Aggregate all the frame captions to generate a context\n",
        "    video_sentences = '\\n'.join(captions_dict[video_id])\n",
        "\n",
        "    # Define the instruction prompt with (1) task description, (2) in-context examples, and (3) input sentence.\n",
        "    prompt = f\"\"\"\n",
        "    Instruction:\n",
        "    Create a summary sentence that aggregate the meaning of all the sentences provided in the context. The sentences in the context are in cronological order. Provide a concise summary using only the information provided in the context.\n",
        "\n",
        "    Context:\n",
        "    The two kids are playing with the cat\n",
        "    The cat is runnning in the living room .\n",
        "    The cat is runnning in the living room .\n",
        "    Two kids are trying to catch a black cat\n",
        "    One child is running in the living room .\n",
        "    The cat is runnning in the living room .\n",
        "    A woman is holding a black cat .\n",
        "    The children and a woman are petting the cat .\n",
        "\n",
        "    Summary:\n",
        "    Two children run into the living room trying to catch a black cat. After a woman catches the cat, they all pet it together .\n",
        "\n",
        "    Context:\n",
        "    {video_sentences}\n",
        "\n",
        "    Summary:\n",
        "    \"\"\"\n",
        "    \n",
        "    # Generate aggregate captions using the LLM\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        outputs = model.generate(**inputs, repetition_penalty=1.2, max_new_tokens=50)\n",
        "    generated_text = tokenizer.decode(outputs[0][len(inputs.input_ids.squeeze()):], skip_special_tokens=True).strip()\n",
        "    \n",
        "    # Attach the summarized caption to the video\n",
        "    aggregated_captions[video_id] = generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualize the aggregated caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr38Pl4-cafT",
        "outputId": "cb77f116-7482-40a2-dd7e-cf8971ec5941"
      },
      "outputs": [],
      "source": [
        "video_id = list(aggregated_captions.keys())[0]\n",
        "\n",
        "print(\"AGGREGATED:\", aggregated_captions[video_id])\n",
        "print(f\"ORIGINAL CAPTIONS ({len(glob(path.join('data', 'frames',video_id, '*.png')))} frames || {len(captions_dict[video_id])} captions):\")\n",
        "for frame_id, caption in enumerate(captions_dict[video_id]):\n",
        "    print(f'- FRAME {frame_id + 1}: \"{caption}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQN7gL77M2Mk"
      },
      "source": [
        "# Step 3: Project video/caption emebeddings in a low dimensional space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b435gbLgXV3j"
      },
      "outputs": [],
      "source": [
        "!pip install -q umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO5CQohA4hxu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from open_clip.factory import get_tokenizer\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6R_nOlReJgn"
      },
      "outputs": [],
      "source": [
        "# instantiate again COCA\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "  model_name=\"coca_ViT-L-14\",\n",
        "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\",\n",
        "  device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc96Sg06bEft",
        "outputId": "6a314694-7573-4c91-cb20-990e22584649"
      },
      "outputs": [],
      "source": [
        "video_embeddings = []\n",
        "\n",
        "for video_id in aggregated_captions_dict:\n",
        "    video_features = []\n",
        "    frame_files = sorted(glob(f\"{video_frames_dir}/{video_id}/*\"))\n",
        "    for frame_path in frame_files:\n",
        "        im = Image.open(frame_path).convert(\"RGB\")\n",
        "        im = preprocess(im).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(im).float()\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        video_features.append(image_features.cpu().numpy())\n",
        "    \n",
        "    # a video embedding is the average of the frames embeddings\n",
        "    if video_features:  # check if we have frames\n",
        "        video_embeddings.append(np.mean(np.asarray(video_features), axis=0))\n",
        "\n",
        "video_embeddings = np.asarray(video_embeddings).squeeze()\n",
        "print(video_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xscUbd-nazXn",
        "outputId": "90989dfd-d139-48c1-a0c4-52522e9ccfc5"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"coca_ViT-L-14\")\n",
        "\n",
        "caption_embeddings = []\n",
        "\n",
        "for video_id in aggregated_captions_dict:\n",
        "    text_tokens = tokenizer(aggregated_captions_dict[video_id]).to(device)\n",
        "    # the following line fix a bug - handle case where no padding tokens exist\n",
        "    if torch.any(text_tokens == 0):\n",
        "        text_tokens = text_tokens[:, :torch.where(text_tokens == 0)[1][0]]\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    caption_embeddings.append(text_features.cpu().numpy())\n",
        "\n",
        "caption_embeddings = np.asarray(caption_embeddings).squeeze()\n",
        "print(caption_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "_yRGB83ZXePL",
        "outputId": "7e0c0df8-6326-4804-cdbb-e651bd8856af"
      },
      "outputs": [],
      "source": [
        "# instantiate umap\n",
        "reducer = UMAP()\n",
        "\n",
        "# obtain 2d features\n",
        "features_2d = reducer.fit_transform(np.concatenate([video_embeddings, caption_embeddings], 0))\n",
        "\n",
        "# plot 2d features\n",
        "plt.scatter(features_2d[:-len(video_embeddings), 0], features_2d[:-len(video_embeddings), 1], c='tab:blue', label=\"video\")\n",
        "plt.scatter(features_2d[-len(video_embeddings):, 0], features_2d[-len(video_embeddings):, 1], c='tab:red', label=\"text\")\n",
        "# plot lines\n",
        "for i in range(len(video_embeddings)):\n",
        "    plt.plot([features_2d[i, 0], features_2d[len(video_embeddings)+i, 0]], [features_2d[i, 1], features_2d[len(video_embeddings)+i, 1]], c='black', alpha=0.1)\n",
        "\n",
        "plt.xlabel('umap 1')\n",
        "plt.ylabel('umap 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "Z2wULIw9RTH2",
        "outputId": "3702c2cd-a98b-49fb-a88b-b3b92b2466b7"
      },
      "outputs": [],
      "source": [
        "# define svd\n",
        "def svd(X, n_components=2):\n",
        "    U, S, Vt = np.linalg.svd(X)\n",
        "    return U[:, :n_components] * S[:n_components]\n",
        "\n",
        "# obtain 2d features\n",
        "features_2d = svd(np.concatenate([video_embeddings, caption_embeddings], 0))\n",
        "\n",
        "# plot 2d features\n",
        "plt.scatter(features_2d[:-len(video_embeddings), 0], features_2d[:-len(video_embeddings), 1], c='tab:blue', label=\"video\")\n",
        "plt.scatter(features_2d[-len(video_embeddings):, 0], features_2d[-len(video_embeddings):, 1], c='tab:red', label=\"text\")\n",
        "# plot lines\n",
        "for i in range(len(video_embeddings)):\n",
        "    plt.plot([features_2d[i, 0], features_2d[len(video_embeddings)+i, 0]], [features_2d[i, 1], features_2d[len(video_embeddings)+i, 1]], c='black', alpha=0.1)\n",
        "\n",
        "plt.xlabel('svd 1')\n",
        "plt.ylabel('svd 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
